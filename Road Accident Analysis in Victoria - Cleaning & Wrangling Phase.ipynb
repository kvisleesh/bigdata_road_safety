{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Accident Analysis in Victoria\n",
    "*FIT5202 Data processing for Big data* | **Phase 2: Data Wrangling and Exploration** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"toc\"></a>\n",
    "## Table of Contents\n",
    "\n",
    "* [Introduction](#intro)\n",
    "* [Import library](#import)\n",
    "* [Initialize SparkSession](#initialize)\n",
    "* [Data Loading and Preparation](#data_loading)\n",
    "* [Data Exploration](#data_exploration)\n",
    "    * [accident](#explore_accident)\n",
    "    * [accident_chainage](#explore_accident_chainage)\n",
    "    * [accident_event](#explore_accident_event)\n",
    "    * [accident_location](#explore_accident_location)\n",
    "    * [atmospheric_condition](#explore_atmosp_cond)\n",
    "    * [node](#explore_node)\n",
    "    * [node_id](#explore_node_id)\n",
    "    * [person](#explore_person)\n",
    "    * [surface_condition](#explore_surface_cond)\n",
    "    * [subdca](#explore_subdca)\n",
    "    * [vehicle](#explore_vehicle)\n",
    "* [Data Cleaning & Transformation](#data_transformation)\n",
    "    * [Part 1: Clean and Transform individual dataframe/dataset](#transform_part1) \n",
    "        * [atmospheric_condition](#transform_atmosp_cond) \n",
    "        * [surface_condition](#transform_surface_cond) \n",
    "        * [person](#transform_person) \n",
    "        * [node](#transform_node) \n",
    "        * [accident](#transform_accident) \n",
    "        * [vehicle](#transform_vehicle) \n",
    "    * [Part 2: Join all the clean dataframes into a joined dataframe](#transform_part2) \n",
    "    * [Part 3: Perform exploration, cleaning & transformation on the joined dataframe](#transform_part3)    \n",
    "    * [Summary](#transformation_summary)\n",
    "* [Export clean dataset](#export_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"intro\"></a>\n",
    "<div style=\"background:rgba(0,80,80,0.2);padding:10px;border-radius:4px\"><h2>Introduction</h2>\n",
    "<hr/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to deliver a predicting modelling that will inform road safety partners in minimising the risk associated with roads, vehicles, speed and drivers within the community. The model will provide insight on situations where road accidents are more likely to occur in order to inform policy makers to make better decisions on how to reduce the incidence of serious injury and/or death resulting from road accidents.   \n",
    "\n",
    "------\n",
    "\n",
    "In this exercise, data cleansing and data wrangling are performed on the 11 files downloaded from https://www.data.vic.gov.au/ website. See the brief description of each file below:\n",
    "\n",
    "[DATASETS](https://discover.data.vic.gov.au/dataset/crash-statistics) :\n",
    "  1. **accident** - details of each accident incident such as date\n",
    "  2. **accident_chainage** - details of route and chainage data such as route number, route link no etc \n",
    "  3. **accident_event** - details of events occured in each accident such as collision, fell from vehicle etc. \n",
    "  4. **accident_location** - details of local location of the accident such as road name, type etc\n",
    "  5. **atmospheric_condition** - describes the weather conditions during accident.\n",
    "  6. **node** - provides area where the accident occured such as region, LGA etc.\n",
    "  7. **node_id_complex_int_id** - this is a table linking the accident and node table.\n",
    "  8. **person** - details of each person involved in each accident such as age, whether seatbelt is worn etc\n",
    "  9. **surface_condition** - details of the surface conditions of the road such as dry, wet, muddy etc\n",
    "  10. **subdca** - details of how the accident occured. Refer to the [VicRoads Crash Stats User Guide and Appendices](https://data.vicroads.vic.gov.au/metadata/Crashstats_user_Guide_and_Appendices.pdf) for visual explanation referenced by the code. \n",
    "  11. **vehicle** - details of each vehicle involved in each accident such as make, model, year\n",
    "\n",
    "Some of the cleansing and wrangling tasks performed are:\n",
    "* correct data types\n",
    "* remove duplicates\n",
    "* fill in missing values\n",
    "* data validation\n",
    "* remove duplicates\n",
    "* remove outliers\n",
    "\n",
    "The tables will be joined to form a master table. Further exploration, cleansing and transformation are done to a desired format and shape in preparation for running Machine Learning models in the next phase. A careful selection of the relevant variables is justified in detail. This is to ensure a high probability prediction from the Machine Learning model.\n",
    "\n",
    "**META-DATA REFERENCES:**  \n",
    "\n",
    "[VicRoads Crash Stats User Guide and Appendices](https://data.vicroads.vic.gov.au/metadata/Crashstats_user_Guide_and_Appendices.pdf)   \n",
    "[Metadata for Crash Stats Data Extract](https://data.vicroads.vic.gov.au/Metadata/Crash%20Stats%20-%20Data%20Extract%20-%20Open%20Data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"import\"></a>\n",
    "<div style=\"background:rgba(0,80,80,0.2);padding:10px;border-radius:4px\"><h2>Import library</h2>\n",
    "<hr/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, countDistinct, isnan, when, count, regexp_replace, regexp_extract, substring, trim, first, round, spark_partition_id, asc, desc, broadcast\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, BooleanType, DoubleType,TimestampType\n",
    "\n",
    "\n",
    "## display all columns for Pandas Dataframe\n",
    "# TURN ON for exploration phase. T\n",
    "## TURN OFF this options before submission for better performance\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"initialize\"></a>\n",
    "<div style=\"background:rgba(0,80,80,0.2);padding:10px;border-radius:4px\"><h2>Initialize Spark Session</h2>\n",
    "<hr/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# Set the options to run Spark in local mode with different processors requirements.\n",
    "master = \"local[*]\"\n",
    "\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Road Accident Analysis in Victoria\"\n",
    "\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "# test with force parameters with higher machine specs\n",
    "# spark_conf = SparkConf().setMaster(master).setAppName(app_name).setAll([('spark.executor.memory', '12g'), ('spark.executor.cores', '6'), ('spark.cores.max', '6'), ('spark.driver.memory','12g')])\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get SparkSession info\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"data_loading\"></a>\n",
    "<div style=\"background:rgba(0,80,80,0.2);padding:10px;border-radius:4px\"><h2>Data loading and Preparation</h2>\n",
    "<hr/>\n",
    "    \n",
    "This section focuses on loading the files. The tasks involves:  \n",
    "\n",
    "- specifying the data schema for the dataframes/datasets, to ensure data types are correct for any later tranformation and calculation operations. Datatypes were identified after initial pass of data without specifying schema originally and inspecting the files\n",
    "- loading files  \n",
    "- understanding structures, including number of attributes/columns and number of records/row\n",
    "- define schemas in advance allowing the correct loading of values for next stages of exploration.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import 11-files for exploration ##\n",
    "\n",
    "# Import: ACCIDENT.csv\n",
    "\n",
    "# Rename Any columns with lower case / spaces in them.\n",
    "accident_schema = StructType() \\\n",
    "    .add(\"ACCIDENT_NO\",StringType()) \\\n",
    "    .add(\"ACCIDENTDATE\",StringType()) \\\n",
    "    .add(\"ACCIDENTTIME\",StringType()) \\\n",
    "    .add(\"ACCIDENT_TYPE\",IntegerType()) \\\n",
    "    .add(\"ACCIDENT_TYPE_DESC\",StringType()) \\\n",
    "    .add(\"DAY_OF_WEEK\",IntegerType()) \\\n",
    "    .add(\"DAY_OF_WEEK_DESC\",StringType()) \\\n",
    "    .add(\"DCA_CODE\",IntegerType()) \\\n",
    "    .add(\"DCA_DESC\",StringType()) \\\n",
    "    .add(\"DIRECTORY\",StringType()) \\\n",
    "    .add(\"EDITION\",StringType()) \\\n",
    "    .add(\"PAGE\",StringType()) \\\n",
    "    .add(\"GRID_REFERENCE_X\",StringType()) \\\n",
    "    .add(\"GRID_REFERENCE_Y\",StringType()) \\\n",
    "    .add(\"LIGHT_CONDITION\",IntegerType()) \\\n",
    "    .add(\"LIGHT_CONDITION_DESC\",StringType()) \\\n",
    "    .add(\"NODE_ID\",IntegerType()) \\\n",
    "    .add(\"NO_OF_VEHICLES\",IntegerType()) \\\n",
    "    .add(\"NO_PERSONS\",IntegerType()) \\\n",
    "    .add(\"NO_PERSONS_INJ_2\",IntegerType()) \\\n",
    "    .add(\"NO_PERSONS_INJ_3\",IntegerType()) \\\n",
    "    .add(\"NO_PERSONS_KILLED\",IntegerType()) \\\n",
    "    .add(\"NO_PERSONS_NOT_INJ\",IntegerType()) \\\n",
    "    .add(\"POLICE_ATTEND\",IntegerType()) \\\n",
    "    .add(\"ROAD_GEOMETRY\",IntegerType()) \\\n",
    "    .add(\"ROAD_GEOMETRY_DESC\",StringType()) \\\n",
    "    .add(\"SEVERITY\",IntegerType()) \\\n",
    "    .add(\"SPEED_ZONE\",StringType()) \n",
    "\n",
    "\n",
    "df_accident = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .schema(accident_schema)\\\n",
    "            .load('data/ACCIDENT.csv')\\\n",
    "            .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import: ACCIDENT_CHAINAGE.csv\n",
    "accident_chainage_schema = StructType() \\\n",
    "    .add(\"NODE_ID\",IntegerType())\\\n",
    "    .add(\"ROUTE_NO\", IntegerType())\\\n",
    "    .add(\"CHAINAGE_SEQ\", IntegerType())\\\n",
    "    .add(\"ROUTE_LINK_NO\",IntegerType())\\\n",
    "    .add(\"CHAINAGE\",IntegerType())\n",
    "\n",
    "df_accident_chainage = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .schema(accident_chainage_schema)\\\n",
    "            .load('data/ACCIDENT_CHAINAGE.csv')\\\n",
    "            .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import: ACCIDENT_EVENT.csv\n",
    "\n",
    "accident_event_schema = StructType()\\\n",
    "    .add(\"ACCIDENT_NO\",StringType())\\\n",
    "    .add(\"EVENT_SEQ_NO\",IntegerType())\\\n",
    "    .add(\"EVENT_TYPE\", StringType())\\\n",
    "    .add(\"EVENT_TYPE_DESC\", StringType())\\\n",
    "    .add(\"VEHICLE_1_ID\", StringType())\\\n",
    "    .add(\"VEHICLE_1_COLL_PT\", StringType())\\\n",
    "    .add(\"VEHICLE_1_COLL_PT_DESC\", StringType())\\\n",
    "    .add(\"VEHICLE_2_ID\", StringType())\\\n",
    "    .add(\"VEHICLE_2_COLL_PT\", StringType())\\\n",
    "    .add(\"VEHICLE_2_COLL_PT_DESC\", StringType())\\\n",
    "    .add(\"PERSON_ID\", StringType())\\\n",
    "    .add(\"OBJECT_TYPE\", StringType())\\\n",
    "    .add(\"OBJECT_TYPE_DESC\", StringType())\n",
    "\n",
    "df_accident_event = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .schema(accident_event_schema)\\\n",
    "            .load('data/ACCIDENT_EVENT.csv')\\\n",
    "            .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import: ACCIDENT_LOCATION.csv\n",
    "accident_location_schema = StructType()\\\n",
    "    .add(\"ACCIDENT_NO\",StringType())\\\n",
    "    .add(\"NODE_ID\", IntegerType())\\\n",
    "    .add(\"ROAD_ROUTE_1\", IntegerType())\\\n",
    "    .add(\"ROAD_NAME\", StringType())\\\n",
    "    .add(\"ROAD_TYPE\", StringType())\\\n",
    "    .add(\"ROAD_NAME_INT\", StringType())\\\n",
    "    .add(\"ROAD_TYPE_INT\", StringType())\\\n",
    "    .add(\"DISTANCE_LOCATION\", IntegerType())\\\n",
    "    .add(\"DIRECTION_LOCATION\", StringType())\\\n",
    "    .add(\"NEAREST_KM_POST\", StringType())\\\n",
    "    .add(\"OFF_ROAD_LOCATION\", StringType())\n",
    "\n",
    "df_accident_location = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .load('data/ACCIDENT_LOCATION.csv')\\\n",
    "            .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import: ATMOSPHERIC_COND.csv\n",
    "atmos_cond_shema = StructType()\\\n",
    "    .add(\"ACCIDENT_NO\",StringType())\\\n",
    "    .add(\"ATMOSPH_COND\",StringType())\\\n",
    "    .add(\"ATMOSPH_COND_SEQ\",IntegerType())\\\n",
    "    .add(\"ATMOSPH_COND_DESC\",StringType())\n",
    "\n",
    "\n",
    "df_atmospheric_cond = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .schema(atmos_cond_shema)\\\n",
    "            .load('data/ATMOSPHERIC_COND.csv')\\\n",
    "            .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import: NODE.csv\n",
    "\n",
    "node_schema = StructType()\\\n",
    "    .add(\"ACCIDENT_NO\", StringType())\\\n",
    "    .add(\"NODE_ID\", IntegerType())\\\n",
    "    .add(\"NODE_TYPE\", StringType())\\\n",
    "    .add(\"VICGRID94_X\", DoubleType())\\\n",
    "    .add(\"VICGRID94_Y\", DoubleType())\\\n",
    "    .add(\"LGA_NAME\", StringType())\\\n",
    "    .add(\"LGA_NAME_ALL\", StringType())\\\n",
    "    .add(\"REGION_NAME\", StringType())\\\n",
    "    .add(\"DEG_URBAN_NAME\", StringType())\\\n",
    "    .add(\"LAT\", DoubleType())\\\n",
    "    .add(\"LONG\", DoubleType())\\\n",
    "    .add(\"POSTCODE_NO\", IntegerType())\n",
    "\n",
    "df_node = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .schema(node_schema)\\\n",
    "            .load('data/NODE.csv')\\\n",
    "            .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import: NODE_ID_COMPLEX_INT_ID.csv\n",
    "\n",
    "node_id_schema = StructType()\\\n",
    "    .add(\"ACCIDENT_NO\",StringType())\\\n",
    "    .add(\"NODE_ID\",IntegerType())\\\n",
    "    .add(\"COMPLEX_INT_NO\",IntegerType())\n",
    "\n",
    "\n",
    "df_node_id= spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .schema(node_id_schema)\\\n",
    "            .load('data/NODE_ID_COMPLEX_INT_ID.csv')\\\n",
    "            .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import: PERSON.csv\n",
    "\n",
    "person_schema = StructType()\\\n",
    "    .add(\"ACCIDENT_NO\",StringType())\\\n",
    "    .add(\"PERSON_ID\",StringType())\\\n",
    "    .add(\"VEHICLE_ID\",StringType())\\\n",
    "    .add(\"SEX\",StringType())\\\n",
    "    .add(\"AGE\",IntegerType())\\\n",
    "    .add(\"AGE_GROUP\",StringType())\\\n",
    "    .add(\"INJ_LEVEL\",IntegerType())\\\n",
    "    .add(\"INJ_LEVEL_DESC\",StringType())\\\n",
    "    .add(\"SEATING_POSITION\",StringType())\\\n",
    "    .add(\"HELMET_BELT_WORN\",IntegerType())\\\n",
    "    .add(\"ROAD_USER_TYPE\",IntegerType())\\\n",
    "    .add(\"ROAD_USER_TYPE_DESC\",StringType())\\\n",
    "    .add(\"LICENCE_STATE\",StringType())\\\n",
    "    .add(\"PEDEST_MOVEMENT\",IntegerType())\\\n",
    "    .add(\"POSTCODE\",IntegerType())\\\n",
    "    .add(\"TAKEN_HOSPITAL\",StringType())\\\n",
    "    .add(\"EJECTED_CODE\",IntegerType())\n",
    "\n",
    "df_person = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .schema(person_schema)\\\n",
    "            .load('data/PERSON.csv')\\\n",
    "            .cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import: ROAD_SURFACE_COND.csv\n",
    "\n",
    "surface_cond_schema = StructType()\\\n",
    "    .add(\"ACCIDENT_NO\", StringType())\\\n",
    "    .add(\"SURFACE_COND\", IntegerType())\\\n",
    "    .add(\"SURFACE_COND_DESC\", StringType())\\\n",
    "    .add(\"SURFACE_COND_SEQ\",IntegerType())\n",
    "\n",
    "df_surface_cond = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .schema(surface_cond_schema)\\\n",
    "            .load('data/ROAD_SURFACE_COND.csv')\\\n",
    "            .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import: SUBDCA.csv\n",
    "\n",
    "subdca_schema = StructType()\\\n",
    "    .add(\"ACCIDENT_NO\", StringType())\\\n",
    "    .add(\"SUB_DCA_CODE\", StringType())\\\n",
    "    .add(\"SUB_DCA_SEQ\", IntegerType())\\\n",
    "    .add(\"SUB_DCA_CODE_DESC\",StringType())\n",
    "\n",
    "df_subdca = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .schema(subdca_schema)\\\n",
    "            .load('data/SUBDCA.csv')\\\n",
    "            .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import: VEHICLE.csv\n",
    "vehicle_schema = StructType()\\\n",
    "    .add(\"ACCIDENT_NO\", StringType())\\\n",
    "    .add(\"VEHICLE_ID\", StringType())\\\n",
    "    .add(\"VEHICLE_YEAR_MANUF\", StringType())\\\n",
    "    .add(\"VEHICLE_DCA_CODE\", StringType())\\\n",
    "    .add(\"INITIAL_DIRECTION\", StringType())\\\n",
    "    .add(\"ROAD_SURFACE_TYPE\", IntegerType())\\\n",
    "    .add(\"ROAD_SURFACE_TYPE_DESC\", StringType())\\\n",
    "    .add(\"REG_STATE\", StringType())\\\n",
    "    .add(\"VEHICLE_BODY_STYLE\", StringType())\\\n",
    "    .add(\"VEHICLE_MAKE\", StringType())\\\n",
    "    .add(\"VEHICLE_MODEL\", StringType())\\\n",
    "    .add(\"VEHICLE_POWER\", IntegerType())\\\n",
    "    .add(\"VEHICLE_TYPE\", StringType())\\\n",
    "    .add(\"VEHICLE_TYPE_DESC\", StringType())\\\n",
    "    .add(\"VEHICLE_WEIGHT\", IntegerType())\\\n",
    "    .add(\"CONSTRUCTION_TYPE\", StringType())\\\n",
    "    .add(\"FUEL_TYPE\", StringType())\\\n",
    "    .add(\"NO_OF_WHEELS\", IntegerType())\\\n",
    "    .add(\"NO_OF_CYLINDERS\", IntegerType())\\\n",
    "    .add(\"SEATING_CAPACITY\", IntegerType())\\\n",
    "    .add(\"TARE_WEIGHT\", IntegerType())\\\n",
    "    .add(\"TOTAL_NO_OCCUPANTS\", IntegerType())\\\n",
    "    .add(\"CARRY_CAPACITY\", IntegerType())\\\n",
    "    .add(\"CUBIC_CAPACITY\", IntegerType())\\\n",
    "    .add(\"FINAL_DIRECTION\", StringType())\\\n",
    "    .add(\"DRIVER_INTENT\", StringType())\\\n",
    "    .add(\"VEHICLE_MOVEMENT\", StringType())\\\n",
    "    .add(\"TRAILER_TYPE\", StringType())\\\n",
    "    .add(\"VEHICLE_COLOUR_1\", StringType())\\\n",
    "    .add(\"VEHICLE_COLOUR_2\", StringType())\\\n",
    "    .add(\"CAUGHT_FIRE\", StringType())\\\n",
    "    .add(\"INITIAL_IMPACT\", StringType())\\\n",
    "    .add(\"LAMPS\", IntegerType())\\\n",
    "    .add(\"LEVEL_OF_DAMAGE\", IntegerType())\\\n",
    "    .add(\"OWNER_POSTCODE\", StringType())\\\n",
    "    .add(\"TOWED_AWAY_FLAG\", StringType())\\\n",
    "    .add(\"TRAFFIC_CONTROL\", StringType())\\\n",
    "    .add(\"TRAFFIC_CONTROL_DESC\", StringType())\n",
    "\n",
    "df_vehicle = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .schema(vehicle_schema)\\\n",
    "            .load('data/VEHICLE.csv')\\\n",
    "            .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This section is turned off for assignment submission, uncomment as required\n",
    "\n",
    "# This section was used \n",
    "# to verify the first loading (without schema) \n",
    "# to verify the loading result (after schema defined)\n",
    "\n",
    "# check the Structures for all dataframe \n",
    "# print(\"df_accident\")\n",
    "# df_accident.printSchema()\n",
    "\n",
    "# print(\"df_accident_chainage\")\n",
    "# df_accident_chainage.printSchema()\n",
    "\n",
    "# print(\"df_accident_event\")\n",
    "# df_accident_event.printSchema()\n",
    "\n",
    "# print(\"df_accident_location\")\n",
    "# df_accident_location.printSchema()\n",
    "\n",
    "# print(\"df_atmospheric_cond\")\n",
    "# df_atmospheric_cond.printSchema()\n",
    "\n",
    "# print(\"df_node\")\n",
    "# df_node.printSchema()\n",
    "\n",
    "# print(\"df_node_id\")\n",
    "# df_node_id.printSchema()\n",
    "\n",
    "# print(\"df_person\")\n",
    "# df_person.printSchema()\n",
    "\n",
    "# print(\"df_surface_cond\")\n",
    "# df_surface_cond.printSchema()\n",
    "\n",
    "# print(\"df_subdca\")\n",
    "# df_subdca.printSchema()\n",
    "\n",
    "# print(\"df_vehicle\")\n",
    "# df_vehicle.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shape_initial = spark.createDataFrame(\n",
    "    [\n",
    "        (\"df_accident\", \"Number of Records\", df_accident.count() ), \n",
    "        (\"df_accident\", \"Number of Columns\", len(df_accident.columns)), \n",
    "        (\"df_accident_chainage\", \"Number of Records\", df_accident_chainage.count() ), \n",
    "        (\"df_accident_chainage\", \"Number of Columns\", len(df_accident_chainage.columns)), \n",
    "        (\"df_accident_event\", \"Number of Records\", df_accident_event.count() ), \n",
    "        (\"df_accident_event\", \"Number of Columns\", len(df_accident_event.columns)), \n",
    "        (\"df_accident_location\", \"Number of Records\", df_accident_location.count() ), \n",
    "        (\"df_accident_location\", \"Number of Columns\", len(df_accident_location.columns)), \n",
    "        (\"df_atmospheric_cond\", \"Number of Records\", df_atmospheric_cond.count() ), \n",
    "        (\"df_atmospheric_cond\", \"Number of Columns\", len(df_atmospheric_cond.columns)), \n",
    "        (\"df_node\", \"Number of Records\", df_node.count() ), \n",
    "        (\"df_node\", \"Number of Columns\", len(df_node.columns)), \n",
    "        (\"df_node_id\", \"Number of Records\", df_node_id.count() ), \n",
    "        (\"df_node_id\", \"Number of Columns\", len(df_node_id.columns)), \n",
    "        (\"df_person\", \"Number of Records\", df_person.count() ), \n",
    "        (\"df_person\", \"Number of Columns\", len(df_person.columns)), \n",
    "        (\"df_surface_cond\", \"Number of Records\", df_surface_cond.count() ), \n",
    "        (\"df_surface_cond\", \"Number of Columns\", len(df_surface_cond.columns)), \n",
    "        (\"df_subdca\", \"Number of Records\", df_subdca.count() ), \n",
    "        (\"df_subdca\", \"Number of Columns\", len(df_subdca.columns)), \n",
    "        (\"df_vehicle\", \"Number of Records\", df_vehicle.count() ), \n",
    "        (\"df_vehicle\", \"Number of Columns\", len(df_vehicle.columns)), \n",
    "    ],\n",
    "    [\"DATAFRAME\", \"SHAPE\",\"INITIAL RESULTS\"] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the dataframe\n",
    "df_shape_initial.groupBy(\"DATAFRAME\").pivot(\"SHAPE\").sum().sort(\"Number of Records\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"data_exploration\"></a>\n",
    "<div style=\"background:rgba(0,80,80,0.2);padding:10px;border-radius:4px\"><h2>Data Exploration</h2>\n",
    "<hr/>\n",
    "\n",
    "This section mainly focuses on data exploration on individual datasets.The following activities, but not limited to, will be performed:\n",
    "- perform summary statitistic, including for each columns, number of records, number of unique records, type of values/inputs  \n",
    "- understand the records  \n",
    "- checking any missing/null values   \n",
    "- checking data distribution for each column   \n",
    "- checking any duplicates  \n",
    "- checking any inconsistency/invalid data  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"explore_accident\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:10px\"><strong> Dataset: </strong> <strongstyle=\"color:blue\"> accident</strong></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Summary Statistics: # df_accident\n",
    "df_accident.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some data\n",
    "#df_accident.show(2,vertical=True,truncate=False)\n",
    "df_accident.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the columns relationship:  \n",
    "`NO_PERSONS` = `NO_PERSONS_INJ_2` (Injury level 2) + `NO_PERSONS_INJ_3` (Injury level 3) + `NO_PERSONS_KILLED` + `NO_PERSONS_NOT_INJ` (No Injury)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing/null values : df_accident\n",
    "df_accident.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_accident.columns]).show(2,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DIRECTORY`, `EDITION`, `PAGE`, `GRID_REFERENCE_X`,`GRID_REFERENCE_Y` (map related data) contain missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue records for each column\n",
    "df_accident.select([countDistinct(c).alias(c) for c in df_accident.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ACCIDENT_NO` is a unique record.  \n",
    "`SEVERITY` is classified for each accident.  \n",
    "`DIRECTORY`, `EDITION`, `PAGE`, `GRID_REFERENCE_X`,`GRID_REFERENCE_Y` (map related data) contain missing value.  \n",
    "There are many key-value-pairs provided:  \n",
    "- `ACCIDENT_TYPE` & `ACCIDENT_TYPE_DESC`  \n",
    "- `DCA_CODE` & `DCA_DESC`  \n",
    "- `LIGHT_CONDITION` & `LIGHT_CONDITION_DESC`  \n",
    "- `ROAD_GEOMETRY` & `ROAD_GEOMETRY_DESC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue values for each column\n",
    "[df_accident.groupBy(c).count().sort(col(\"count\").desc()).show(truncate=False) for c in df_accident.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ACCIDENTTIME` contains trialing space in the text.  \n",
    "`DIRECTORY`, `EDITION`, `PAGE`, `GRID_REFERENCE_X`,`GRID_REFERENCE_Y` (map related data) contain string with \"space\" in the text.  \n",
    "`NODE_ID` contain value -1,-10,-3. Require to explore.\n",
    "`SPEED_ZONE` requires to be updated with more description.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicate\n",
    "df_accident.groupBy(\"ACCIDENT_NO\").count().filter(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no duplicate record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing values: \"ACCIDENT_TYPE\" & \"ACCIDENT_TYPE_DESC\"\n",
    "df_accident.groupBy(\"ACCIDENT_TYPE\",\"ACCIDENT_TYPE_DESC\").count().sort(col(\"ACCIDENT_TYPE\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing values: \"DCA_CODE\" & \"DCA_DESC\"\n",
    "df_accident.groupBy(\"DCA_CODE\",\"DCA_DESC\").count().sort(col(\"DCA_CODE\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing values: \"LIGHT_CONDITION\" & \"LIGHT_CONDITION_DESC\"\n",
    "df_accident.groupBy(\"LIGHT_CONDITION\",\"LIGHT_CONDITION_DESC\").count().sort(col(\"LIGHT_CONDITION\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing values: \"ROAD_GEOMETRY\" & \"ROAD_GEOMETRY_DESC\"\n",
    "df_accident.groupBy(\"ROAD_GEOMETRY\",\"ROAD_GEOMETRY_DESC\").count().sort(col(\"ROAD_GEOMETRY\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing values: DAY_OF_WEEK & DAY_OF_WEEK_DESC\n",
    "df_accident.groupBy(\"DAY_OF_WEEK\",\"DAY_OF_WEEK_DESC\").count().sort(col(\"DAY_OF_WEEK_DESC\")).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few values for `DAY_OF_WEEK` for \"Friday\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check Friday records that are not 5  (select only Day Week Description = 2 and 4, where have lesser records)\n",
    "df_accident.filter((col(\"DAY_OF_WEEK_DESC\") == 'Friday') & (col(\"DAY_OF_WEEK\").isin('2','4')))\\\n",
    "            .select(col('ACCIDENTDATE'),col('DAY_OF_WEEK'),col('DAY_OF_WEEK_DESC')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon checking `ACCIDENTDATE` on calendar, these are Friday. This concludes that incorrect value for `DAY_OF_WEEK`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check validity of Node_ID (foreign key)\n",
    "# check whether node_id in df_accidents exists in df_node\n",
    "cond = [df_node.ACCIDENT_NO == df_accident.ACCIDENT_NO, df_node.NODE_ID == df_accident.NODE_ID] \n",
    "invalid_node_id = df_accident.join(df_node, cond, how='left_anti')\\\n",
    "                            .select(col('ACCIDENT_NO'),col('NODE_ID'))\n",
    "\n",
    "# print the number of invalid NODE_ID\n",
    "print(\"Invalid NODE_ID in df_accident  :\", invalid_node_id.count())\n",
    "\n",
    "# view few invalid info\n",
    "invalid_node_id.show(2,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the validity of the NO_PERSONS\n",
    "# `NO_PERSONS` = `NO_PERSONS_INJ_2` + `NO_PERSONS_INJ_3` + `NO_PERSONS_KILLED` + `NO_PERSONS_NOT_INJ` \n",
    "\n",
    "df_accident.agg(F.sum(col('NO_PERSONS_INJ_2')+col('NO_PERSONS_INJ_3')+col('NO_PERSONS_KILLED')+col('NO_PERSONS_NOT_INJ')-col('NO_PERSONS')).alias('Variance')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which accident involved\n",
    "df_accident.groupBy(\"ACCIDENT_NO\")\\\n",
    ".agg(F.sum(col('NO_PERSONS_INJ_2')+col('NO_PERSONS_INJ_3')+col('NO_PERSONS_KILLED')+col('NO_PERSONS_NOT_INJ')-col('NO_PERSONS')).alias('Variance'))\\\n",
    ".filter(\"Variance <> 0\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the accidents where the statistics of people involved in the accidents are not matching with the formula\n",
    "df_accident.filter(col('ACCIDENT_NO').isin('T20190014255')).show(2,vertical=True,truncate=False)\n",
    "#df_accident.filter(col('ACCIDENT_NO').isin('T20190014255')).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore person dataset for the accident\n",
    "df_person.filter(col('ACCIDENT_NO').isin('T20190014255')).show(3,vertical=True,truncate=False)\n",
    "#df_person.filter(col('ACCIDENT_NO').isin('T20190014255')).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After multiple manual validation, there are 234-accidents have incorrect statistics recorded in the datasets. When joining the tables, columns (`NO_PERSONS` = `NO_PERSONS_INJ_2` + `NO_PERSONS_INJ_3` + `NO_PERSONS_KILLED` + `NO_PERSONS_NOT_INJ`)  need to be adjusted or dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'> _SUMMARY_ </font>**  \n",
    "* <strong> **Duplicate** </strong>: No duplicate records.\n",
    "* <strong> **Missing values/null** </strong>:  \n",
    "    `DIRECTORY`, `EDITION`, `PAGE`,\t`GRID_REFERENCE_X`,\t`GRID_REFERENCE_Y` (map/location related info) have plenty of missing.  \n",
    "* <strong> **Outliers** </strong>:  \n",
    "* <strong> **Data Consistency/Validity** </strong>:  \n",
    "    `ACCIDENT_TYPE`, `ACCIDENT_TYPE_DESC` are paired correctly.  \n",
    "    `DCA_CODE`, `DCA_DESC` are paired correctly.  \n",
    "    `LIGHT_CONDITION`, `LIGHT_CONDITION_DESC` are paired correctly.  \n",
    "    `ROAD_GEOMETRY`, `ROAD_GEOMETRY_DESC` are paired correctly.  \n",
    "    `DAY_OF_WEEK`, `DAY_OF_WEEK_DESC` are paired **incorrectly**.  \n",
    "    `NODE_ID` has **invalid** Node_ID that are not exist in df_node.  \n",
    "    `NO_PERSONS` = `NO_PERSONS_INJ_2` + `NO_PERSONS_INJ_3` + `NO_PERSONS_KILLED` + `NO_PERSONS_NOT_INJ` has incorrect statistics compared to df_person.\n",
    "* <strong> **Other** </strong>:  \n",
    "    `POLICE_ATTEND` has no description. Only numerical numbers are provided without explanation.  \n",
    "    `SEVERITY` has no description. Only numerical numbers are provided without explanation  \n",
    "    `ACCIDENTTIME` potentially can group into different time period (Busy hours etc.)\n",
    "    `SPEED_ZONE` has no description for group 777,888,999. \n",
    "    \n",
    "**<font color='blue'> _CLEANING & TRANSFORMATION ACTIVITIES_ </font>**  \n",
    "    `DAY_OF_WEEK` - align to a single index or drop the column  \n",
    "    `POLICE_ATTEND`,`SEVERITY`,`SPEED_ZONE` -  extend the description  \n",
    "    `NODE_ID` - remove invalid node_id or remove records  \n",
    "    `ACCIDENTTIME` - group into different attributes (Peak, non-peak etc)   \n",
    "    `NO_PERSONS`, `NO_PERSONS_INJ_2` + `NO_PERSONS_INJ_3` + `NO_PERSONS_KILLED` + `NO_PERSONS_NOT_INJ`: either has to be adjusted or dropped.\n",
    "    Require to fine-tune the performance in join operation\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"explore_accident_chainage\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:10px\"><strong> Dataset: </strong> <strongstyle=\"color:blue\"> accident_chainage </strong> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Statistics : df_accident_chainage\n",
    "df_accident_chainage.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some data\n",
    "df_accident_chainage.show(truncate=False)\n",
    "#df_accident_chainage.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Node ID` has different notation than other dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing/null values : df_accident_chainage\n",
    "df_accident_chainage.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_accident_chainage.columns]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue records for each column\n",
    "df_accident_chainage.select([countDistinct(c).alias(c) for c in df_accident_chainage.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check unqiue values for each column\n",
    "[df_accident_chainage.groupBy(c).count().sort(col(\"count\").desc()).show(10) for c in df_accident_chainage.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicate\n",
    "df_accident_chainage.groupBy(\"NODE_ID\", \"CHAINAGE_SEQ\").count().filter(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check validity of Node_ID (foreign key)\n",
    "# check whether node_id in df_accident_chainage  exists in df_node\n",
    "invalid_node_id = df_accident_chainage.join(df_node, df_node['NODE_ID'] == df_accident_chainage[\"NODE_ID\"], how='left_anti')\\\n",
    "                            .select(col('NODE_ID'))\n",
    "\n",
    "# print the number of invalid NODE_ID\n",
    "print(\"Invalid NODE_ID in df_accident_chainage  :\", invalid_node_id.count())\n",
    "\n",
    "# view few invalid info\n",
    "invalid_node_id.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NODE_ID` in df_accident_chainage has different notatation/format in df_node.    \n",
    "Therefore, this dataset is not useful for the modelling since it is unable to look up/join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'> _SUMMARY_ </font>**  \n",
    "* <strong> **Duplicate** </strong>: No duplicate records.\n",
    "* <strong> **Missing values/null** </strong>: all columns have null values used.\n",
    "* <strong> **Data Consistency/Validity** </strong>:  Invalid node_id \n",
    "    \n",
    "**<font color='blue'> _CLEANING & TRANSFORMATION ACTIVITIES_ </font>**  \n",
    "Since the infomation is not useful for the modelling, this dataset will not be used. No further action is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"explore_accident_event\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:10px\"><strong> Dataset: </strong> <strongstyle=\"color:blue\"> accident_event </strong> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Statistics : df_accident_event\n",
    "df_accident_event.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key of the dataset is `ACCIDENT_NO` & `EVENT_SEQ_NO`  \n",
    "Minimun 1-vehicle (`VEHICLE_1_ID`) recorded in 1-accident's event.  \n",
    "Not all events have person infomation recorded(`PERSON_ID`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some data\n",
    "#df_accident_event.show(5,vertical=True,truncate=False)\n",
    "df_accident_event.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing/null values : df_accident_event \n",
    "df_accident_event.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_accident_event.columns]).show(2,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4-missing values for 4-records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue records for each column\n",
    "df_accident_event.select([countDistinct(c).alias(c) for c in df_accident_event.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue values for each column\n",
    "[df_accident_event.groupBy(c).count().sort(col(\"count\").desc()).show(truncate=False) for c in df_accident_event.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many key-value-pairs provided:  \n",
    "  `EVENT_TYPE` & `EVENT_TYPE_DESC`  \n",
    "  `VEHICLE_1_COLL_PT` & `VEHICLE_1_COLL_PT_DESC`  \n",
    "  `VEHICLE_2_COLL_PT` & `VEHICLE_2_COLL_PT_DESC`  \n",
    "  `OBJECT_TYPE` & `OBJECT_TYPE_DESC`   \n",
    "There are foreign key from another dataset:`ACCIDENT_NO` , `VEHICLE_1_ID`, `VEHICLE_2_ID`, `PERSON_ID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check duplicate\n",
    "df_accident_event.groupBy(\"ACCIDENT_NO\", \"EVENT_SEQ_NO\").count().filter(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display records that have missing values:\n",
    "#df_accident_event.filter(col('EVENT_SEQ_NO').isNull()).show(5,truncate=False)\n",
    "df_accident_event.filter(col('EVENT_SEQ_NO').isNull()).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No event is recorded for these `ACCIDENT_NO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether these records with missing values, have other records related to the same ACCIDENT_NO within df_accident_event\n",
    "#df_accident_event.filter(col('ACCIDENT_NO').isin('T20170002776','T20170013375','T20170013384','T20170013459')).show(5,truncate=False)\n",
    "df_accident_event.filter(col('ACCIDENT_NO').isin('T20170002776','T20170013375','T20170013384','T20170013459')).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No other event is recorded for these `ACCIDENT_NO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether these ACCIDENT_NO exist in df_accident\n",
    "#df_accident.filter(col('ACCIDENT_NO').isin('T20170002776','T20170013375','T20170013384','T20170013459')).show(5,truncate=False)\n",
    "df_accident.filter(col('ACCIDENT_NO').isin('T20170002776','T20170013375','T20170013384','T20170013459')).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No event is recorded for `ACCIDENT_NO` ('T20170002776','T20170013375','T20170013384','T20170013459')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing values: EVENT_TYPE & EVENT_TYPE_DESC\n",
    "df_accident_event.groupBy(\"EVENT_TYPE\",\"EVENT_TYPE_DESC\").count().sort(col(\"EVENT_TYPE\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing values: VEHICLE_1_COLL_PT & VEHICLE_1_COLL_PT_DESC\n",
    "df_accident_event.groupBy(\"VEHICLE_1_COLL_PT\",\"VEHICLE_1_COLL_PT_DESC\").count().sort(col(\"VEHICLE_1_COLL_PT\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing values: VEHICLE_2_COLL_PT & VEHICLE_2_COLL_PT_DESC\n",
    "df_accident_event.groupBy(\"VEHICLE_2_COLL_PT\",\"VEHICLE_2_COLL_PT_DESC\").count().sort(col(\"VEHICLE_2_COLL_PT\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether these ACCIDENT_NO == \" \" exist in df_accident\n",
    "df_accident_event.filter(col('VEHICLE_2_COLL_PT_DESC').isin(\" \")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VEHICLE_2_COLL_PT` contains \" \" as values, where in fact is null. Replacement is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing values: \"OBJECT_TYPE\" & \"OBJECT_TYPE_DESC\"\n",
    "df_accident_event.groupBy(\"OBJECT_TYPE\",\"OBJECT_TYPE_DESC\").count().sort(col(\"OBJECT_TYPE\")).show(truncate=False)\n",
    "#df_accident_event.groupBy(\"OBJECT_TYPE\",\"OBJECT_TYPE_DESC\").count().sort(col(\"OBJECT_TYPE\")).limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check ACCIDENT_NO validity (foreign key)\n",
    "\n",
    "# check whether ACCIDENT_NO in df_accident_event_cond exists in df_accidents\n",
    "invalid_accident_id = df_accident_event.join(df_accident, df_accident_event.ACCIDENT_NO == df_accident.ACCIDENT_NO, how='left_anti')\\\n",
    "                            .select(col('ACCIDENT_NO'))\n",
    "\n",
    "# print the number of invalid NODE_ID\n",
    "print(\"Invalid ACCIDENT_NO in df_accident_event :\", invalid_accident_id.count())\n",
    "\n",
    "# view few invalid info\n",
    "invalid_accident_id.show(2,vertical=True,truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'> _SUMMARY_ </font>**  \n",
    "* <strong> **Duplicate** </strong>: No duplicate records.  \n",
    "* <strong> **Missing values/null** </strong>: There are 4-records with no event described. remove these records.  \n",
    "* <strong> **Outliers** </strong>:  \n",
    "* <strong> **Data Consistency/Validity** </strong>:  \n",
    "    `EVENT_TYPE`,`EVENT_TYPE_DESC` are paired correctly.  \n",
    "    `VEHICLE_1_COLL_PT`,`VEHICLE_1_COLL_PT_DESC` are paired correctly.  \n",
    "    `VEHICLE_2_COLL_PT`,`VEHICLE_2_COLL_PT_DESC` are paired correctly. Missing value records required to be cleaned with null value.  \n",
    "    `PERSON_ID`: Missing value records required to be cleaned with null value.  \n",
    "    `OBJECT_TYPE`,`OBJECT_TYPE_DESC` are paired correctly.  \n",
    "* <strong> **Other** </strong>:  \n",
    "    \n",
    "**<font color='blue'> _CLEANING & TRANSFORMATION ACTIVITIES_ </font>**  \n",
    "Since the similar and more high-level infomation is available in accident and vehicle datasets for the modelling, this dataset will not be used. No further action is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"explore_accident_location\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:10px\"><strong> Dataset: </strong> <strongstyle=\"color:blue\"> accident_location </strong> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Statistics : df_accident_location\n",
    "df_accident_location.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except `ACCIDENT_NO` & `NODE_ID`,  all columns contains missing values or space character.  \n",
    "The top records for `NODE_ID` is -1. Further exploration is required.\n",
    "Observing from `min` & `max` value of each column, there are records fill with character space \" \" , negative value (e.g. -1) or interesting value 'Z;;;', '9999','999'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some data\n",
    "#df_accident_location.show(2,vertical=True,truncate=False)\n",
    "df_accident_location.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-accident has multiple `NODE_ID`.  \n",
    "`ROAD_ROUTE` has no relationship with `ROAD_NAME`.  \n",
    "`DIRECTION_LOCATION` can be more descriptive to indicate the direction.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing/null values : df_accident_location\n",
    "#df_accident_location.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_accident_location.columns]).toPandas().transpose()\n",
    "df_accident_location.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_accident_location.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except `ACCIDENT_NO` & `NODE_ID`,  all columns contains missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue records for each column\n",
    "#df_accident_event.select([countDistinct(c).alias(c) for c in df_accident_event.columns]).toPandas().transpose()\n",
    "df_accident_event.select([countDistinct(c).alias(c) for c in df_accident_event.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check unqiue values for each column\n",
    "[df_accident_location.groupBy(c).count().sort(col(\"count\").desc()).show(truncate=False) for c in df_accident_location.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset mainly store the location where the accident happening. \n",
    "There are many records have missing values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicate\n",
    "df_accident_location.groupBy(\"ACCIDENT_NO\", \"NODE_ID\").count().filter(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check route infomation for NODE_ID with negative values\n",
    "df_accident_location.filter(col('NODE_ID').isin('-1','-3','-10')).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the which 'NODE_ID' has no accident location    \n",
    "df_accident_location.filter(col('ROAD_NAME').isNull())\\\n",
    "                    .groupBy('NODE_ID').count().show(15)\n",
    "#                    .groupBy('NODE_ID').count().limit(15).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records with missing accident location should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check ACCIDENT_NO & NODE_ID validity (foreign key)\n",
    "\n",
    "# check whether ACCIDENT_NO & NODE_ID in df_accident_location exists in df_accidents\n",
    "cond = [df_accident_location.ACCIDENT_NO == df_accident.ACCIDENT_NO, df_accident_location.NODE_ID == df_accident.NODE_ID]\n",
    "invalid_accident_node = df_accident_location.join(df_accident, cond , how='left_anti')\\\n",
    "                            .select(col('ACCIDENT_NO'), col('NODE_ID'))\n",
    "\n",
    "# print the number of invalid ACCIDENT_NO & NODE_ID\n",
    "print(\"Invalid ACCIDENT_NO & NODE_ID in df_accident_location :\", invalid_accident_node.count())\n",
    "\n",
    "# view few invalid info\n",
    "invalid_accident_node.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'> _SUMMARY_ </font>**  \n",
    "* <strong> **Duplicate** </strong>: No duplicate records.\n",
    "* <strong> **Missing values/null** </strong>: no accident location provided however it seems ROAD_NAME_INT provides suburb information. records should be removed. \n",
    "    `ROAD_ROUTE_1, ROAD_NAME, ROAD_TYPE, ROAD_NAME_INT, ROAD_TYPE_INT, DISTANCE_LOCATION, DIRECTION_LOCATION, NEAREST_KM_POST, OFF_ROAD_LOCATION` - all have null values\n",
    "* <strong> **Outliers** </strong>:  \n",
    "* <strong> **Data Consistency/Validity** </strong>:  \n",
    "* <strong> **Other** </strong>:  \n",
    "    `DIRECTION_LOCATION` - should fill with more descriptive label      \n",
    "    \n",
    "**<font color='blue'> _CLEANING & TRANSFORMATION ACTIVITIES_ </font>**  \n",
    "Since the similar and more high-level infomation is available in node dataset for the modelling, this dataset will not be used. No further action is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"explore_atmosp_cond\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:10px\"><strong> Dataset: </strong> <strongstyle=\"color:blue\"> atmospheric_cond </strong> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Statistics : df_atmospheric_cond\n",
    "df_atmospheric_cond.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is using composite key: `ACCIDENT_NO` & `ATMOSPH_COND_SEQ` ( to be confirmed in the next exploration).  \n",
    "There are 8-atmospheric conditions: `ATMOSPH_COND` & `Atmosph Cond Desc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some data\n",
    "#df_atmospheric_cond.show(2,vertical=True,truncate=False)\n",
    "df_atmospheric_cond.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing/null values : df_atmospheric_cond\n",
    "df_atmospheric_cond.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_atmospheric_cond.columns]).show(vertical=True,truncate=False)\n",
    "#df_atmospheric_cond.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_atmospheric_cond.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue records for each column\n",
    "df_atmospheric_cond.select([countDistinct(c).alias(c) for c in df_atmospheric_cond.columns]).show(vertical=True,truncate=False)\n",
    "#df_atmospheric_cond.select([countDistinct(c).alias(c) for c in df_atmospheric_cond.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue values for each column\n",
    "[df_atmospheric_cond.groupBy(c).count().sort(col(c)).show(truncate=False) for c in df_atmospheric_cond.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing values & its frequency: ATMOSPH_COND & Atmosph Cond Desc\n",
    "df_atmospheric_cond.groupBy(\"ATMOSPH_COND\",\"ATMOSPH_COND_DESC\").count().sort(col(\"ATMOSPH_COND\")).show(truncate=False)\n",
    "#df_atmospheric_cond.groupBy(\"ATMOSPH_COND\",\"ATMOSPH_COND_DESC\").count().sort(col(\"ATMOSPH_COND\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicate\n",
    "df_atmospheric_cond.groupBy(\"ACCIDENT_NO\", \"ATMOSPH_COND_DESC\").count().filter(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check ACCIDENT_NO validity (foreign key)\n",
    "\n",
    "# check whether ACCIDENT_NO in df_atmospheric_cond exists in df_accidents\n",
    "invalid_accident_id = df_atmospheric_cond.join(df_accident, df_atmospheric_cond.ACCIDENT_NO == df_accident.ACCIDENT_NO, how='left_anti')\\\n",
    "                            .select(col('ACCIDENT_NO'))\n",
    "\n",
    "# print the number of invalid ACCIDENT_NO\n",
    "print(\"Invalid ACCIDENT_NO in df_atmospheric_cond  :\", invalid_accident_id.count())\n",
    "\n",
    "# view few invalid info\n",
    "invalid_accident_id.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'> _SUMMARY_ </font>**  \n",
    "* <strong> **Duplicate** </strong>: No duplicate records.\n",
    "* <strong> **Missing values/null** </strong>: No missing values   \n",
    "* <strong> **Outliers** </strong>:  \n",
    "* <strong> **Data Consistency/Validity** </strong>:  \n",
    "* <strong> **Other** </strong>:  \n",
    "    \n",
    "**<font color='blue'> _CLEANING & TRANSFORMATION ACTIVITIES_ </font>**  \n",
    "1-accident can have many atmospheric conditions. In order to avoid duplicated accidents infomation, this dataset needs to be converted from long to wide where atmospheric conditions form individual columns.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"explore_node\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:10px\"><strong> Dataset: </strong> <strongstyle=\"color:blue\"> node </strong> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Summary Statistics : df_node\n",
    "df_node.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some data\n",
    "df_node.show(2,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing/null values : df_node\n",
    "df_node.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_node.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue records for each column\n",
    "df_node.select([countDistinct(c).alias(c) for c in df_node.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue values for each column & its frequency\n",
    "[df_node.groupBy(c).count().sort(col(c)).show(truncate=False) for c in df_node.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicate records\n",
    "df_node.groupby(\"NODE_ID\",\"ACCIDENT_NO\").agg(count(col(\"ACCIDENT_NO\")).alias('num_of_accidents')).sort(col(\"NODE_ID\"))\\\n",
    "        .filter(col('num_of_accidents') > 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicated records\n",
    "df_node.filter(col('ACCIDENT_NO').isin('T20080047570','T20110035570')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records are duplicted due to different `POSTCODE_NO`. In addition, for the same postcode, `DEG_URBAN_NAME` has different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check DEG_URBAN_NAME\n",
    "df_node.groupBy(\"POSTCODE_NO\",\"LGA_NAME\",\"DEG_URBAN_NAME\").count().sort(col(\"POSTCODE_NO\")).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check records with region_name empty/missing values\n",
    "df_node.filter(col(\"REGION_NAME\")==\" \").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether REGION_NAME exist in the same dataframe based on postcode\n",
    "df_node.filter(col(\"POSTCODE_NO\") == \"3030\").show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of description of \"LGA_NAME\",\"LGA_NAME_ALL\",\"REGION_NAME\"  for each postcode\n",
    "df_node.groupby(\"POSTCODE_NO\",\"LGA_NAME\",\"LGA_NAME_ALL\",\"REGION_NAME\", \"DEG_URBAN_NAME\").count().sort(col(\"POSTCODE_NO\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LGA_NAME_ALL`, `LGA_NAME`, `REGION NAME`, `DEG_URBAN_NAME`: \n",
    "    - consist missing values, \n",
    "    - `LGA_NAME`, `REGION NAME` have different labels or combination for the same `LGA_NAME_ALL`\n",
    "    - for the same `POSTCODE_NO`, it has different `DEG_URBAN_NAME`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check validity of ACCIDENT_NO & NODE_ID\n",
    "\n",
    "# check whether the same ACCIDENT_NO & NODE_ID in df_node exists in df_accident\n",
    "cond = [df_node.ACCIDENT_NO == df_accident.ACCIDENT_NO, df_node.NODE_ID == df_accident.NODE_ID] \n",
    "invalid_accident_node = df_node.join(df_accident, cond, how='left_anti')\\\n",
    "                            .select(col('ACCIDENT_NO'),col('NODE_ID'))\n",
    "\n",
    "# print the number of invalid ACCIDENT_NO & NODE_ID\n",
    "print(\"Invalid ACCIDENT_NO & NODE_ID in df_node  :\", invalid_accident_node.count())\n",
    "\n",
    "# view few invalid info\n",
    "invalid_accident_node.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'> _SUMMARY_ </font>**  \n",
    "* <strong> **Duplicate** </strong>: There are a list of duplicated records due to different `POSTCODE_NO` for the same `ACCIDENT_NO` and `NODE_ID`  \n",
    "* <strong> **Missing values/null** </strong>:  LGA_NAME, REGION NAME have missing values \n",
    "* <strong> **Outliers** </strong>:  no outliers to be detected. All postcodes point to VIC state\n",
    "* <strong> **Data Consistency/Validity** </strong>:  \n",
    "    `LGA_NAME_ALL`, `LGA_NAME`, `REGION NAME`: the same suburb has different `LGA_NAME_ALL`, `LGA_NAME`, or `REGION NAME`\n",
    "    `DEG_URBAN_NAME`: for the same `POSTCODE_NO` has different label for `DEG_URBAN_NAME`. \n",
    "    * Assuming that each `REGION NAME` is has different `LGA_NAME_ALL`, `LGA_NAME` and different pockets or development make up labelled as `DEG_URBAN_NAME` \n",
    "* <strong> **Other** </strong>:  \n",
    "    `NODE_TYPE` requires to be more descriptive label.\n",
    "    \n",
    "**<font color='blue'> _CLEANING & TRANSFORMATION ACTIVITIES_ </font>**  \n",
    "* `LGA_NAME_ALL`, `LGA_NAME`, `REGION NAME`, `DEG_URBAN_NAME` with empty values need to be fill in and descrption needs to be aligned.  \n",
    "* Remove duplicate entries where `ACCIDENT_NO` and `NODE_ID` are the same.\n",
    "* Removing duplicates have also removed the missing values entries.\n",
    "* Update `NODE_TYPE`  with more descriptive labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"explore_node_id\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:10px\"><strong> Dataset: </strong> <strongstyle=\"color:blue\"> node_id</strong> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Statistics : df_node_id\n",
    "df_node_id.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some data\n",
    "df_node_id.show(2,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing/null values : df_node_id\n",
    "df_node_id.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_node_id.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue records for each column\n",
    "df_node_id.select([countDistinct(c).alias(c) for c in df_node_id.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue values for each column\n",
    "[df_node_id.groupBy(c).count().sort(col(c)).show(truncate=False) for c in df_node_id.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicate\n",
    "df_node_id.groupBy(\"ACCIDENT_NO\", \"NODE_ID\",\"COMPLEX_INT_NO\").count().filter(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check validity of ACCIDENT_NO & NODE_ID\n",
    "\n",
    "# check whether the same ACCIDENT_NO & NODE_ID in df_node exists in df_accident\n",
    "cond = [df_node_id.ACCIDENT_NO == df_node.ACCIDENT_NO, df_node_id.NODE_ID == df_node.NODE_ID] \n",
    "invalid_accident_node = df_node_id.join(df_node, cond, how='left_anti')\\\n",
    "                            .select(col('ACCIDENT_NO'),col('NODE_ID'))\n",
    "\n",
    "# print the number of invalid ACCIDENT_NO & NODE_ID\n",
    "print(\"Invalid ACCIDENT_NO & NODE_ID in df_node  :\", invalid_accident_node.count())\n",
    "\n",
    "# view few invalid info\n",
    "invalid_accident_node.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'> _SUMMARY_ </font>**  \n",
    "* <strong> **Duplicate** </strong>: No duplicate records.\n",
    "* <strong> **Missing values/null** </strong>: \n",
    "    `COMPLEX_INT_NO` has significant records with null values.\n",
    "    `NODE_ID` of -1,-3,-10 with unknown previously in dataset df_accident_location, exist in this dataset, where the accident_location are unknown.\n",
    "* <strong> **Outliers** </strong>:  \n",
    "* <strong> **Data Consistency/Validity** </strong>:  \n",
    "    `ACCIDENT_NO` & `NODE_ID`: There are 854-records of data that do not exist in df_node. \n",
    "* <strong> **Other** </strong>:  \n",
    "    \n",
    "**<font color='blue'> _CLEANING & TRANSFORMATION ACTIVITIES_ </font>**  \n",
    "This dataset is not relevant to the modelling. Therefore, no further action is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"explore_person\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:10px\"><strong> Dataset: </strong> <strongstyle=\"color:blue\"> person </strong> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_person.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Statistics: df_person\n",
    "df_person.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# view some data\n",
    "df_person.show(2,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing/null values : df_person\n",
    "df_person.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_person.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue records for each column\n",
    "df_person.select([countDistinct(c).alias(c) for c in df_person.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue values for each column\n",
    "[df_person.groupBy(c).count().sort(col(c)).show(truncate=False) for c in df_person.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicate\n",
    "df_person.groupBy(\"ACCIDENT_NO\", \"PERSON_ID\").count().filter(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing and frequency: correctness of assignment of Age to Age Group\n",
    "df_person.crosstab(\"AGE_GROUP\",\"AGE\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age correctly assigned to the `AGE_GROUP`.  \n",
    "There are 21,629 records with unknown `AGE_GROUP` due to no value in `AGE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing and frequency: check the injury level vs age group\n",
    "df_person.crosstab(\"AGE_GROUP\",\"INJ_LEVEL_DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are predicting the severity road accident, it it essential to have infomation provided on Injury level. For records with unknown injury with unknown `Age Group`, records can consider to be removed from datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing and frequency: \n",
    "df_person.crosstab(\"SEX\",\"INJ_LEVEL_DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approx. 4% of records Gender/`Sex` is not known. Most of the person(s) are not injured (`Not injured`).  \n",
    "The fields with `Sex = U` can fill with different genders based on distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing and frequency\n",
    "df_person.crosstab(\"ROAD_USER_TYPE_DESC\",\"INJ_LEVEL_DESC\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing and frequency: \n",
    "df_person.crosstab(\"TAKEN_HOSPITAL\",\"INJ_LEVEL_DESC\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing and frequency: \n",
    "df_person.crosstab(\"HELMET_BELT_WORN\",\"INJ_LEVEL_DESC\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing and frequency: \n",
    "df_person.crosstab(\"EJECTED_CODE\",\"INJ_LEVEL_DESC\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing and frequency\n",
    "df_person.crosstab(\"ROAD_USER_TYPE_DESC\",\"AGE_GROUP\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the info for person with Unknown injury\n",
    "df_person.filter(col(\"INJ_LEVEL_DESC\")==\"Unknown\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknown `INJ_LEVEL`, mainly `ROAD_USER_TYPE_DESC` is 'Driver', with unknown `SEX`, and blank or unknown on various attributes (e.g. `SEATING_POSITION`, `HELMET_BELT_WORN`,`TAKEN_HOSPITAL` etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check ACCIDENT_NO validity (foreign key)\n",
    "\n",
    "# check whether ACCIDENT_NO in df_person exists in df_accidents\n",
    "invalid_accident_id = df_person.join(df_accident, df_person.ACCIDENT_NO == df_accident.ACCIDENT_NO, how='left_anti')\\\n",
    "                            .select(col('ACCIDENT_NO'))\n",
    "\n",
    "# print the number of invalid NODE_ID\n",
    "print(\"Invalid ACCIDENT_NO in df_person  :\", invalid_accident_id.count())\n",
    "\n",
    "# view few invalid info\n",
    "invalid_accident_id.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'> _SUMMARY_ </font>**  \n",
    "* <strong> **Duplicate** </strong>: No duplicate records.\n",
    "* <strong> **Missing values/null** </strong>:   \n",
    "    `VEHICLE_ID`, `SEX`, `INJ_LEVEL`, `SEATING_POSITION`, `HELMET_BELT_WORN`, `ROAD_USER_TYPE`, `LICENCE_STATE`, `PEDEST_MOVEMENT`, `TAKEN_HOSPITAL`, `EJECTED_CODE` contain missing values represent with character space.\n",
    "\n",
    "* <strong> **Outliers** </strong>:  \n",
    "* <strong> **Data Consistency/Validity** </strong>:  \n",
    "* `INJ_LEVEL_DESC` is validated with `INJ_LEVEL`\n",
    "* `AGE_GROUP` is validated with `AGE`\n",
    "* `AGE_GROUP` is reclassified to sensible age range\n",
    "* <strong> **Other** </strong>:  \n",
    "    `SEX`, `SEATING_POSITION`, `HELMET_BELT_WORN`,`TAKEN_HOSPITAL` & `EJECTED_CODE`: provide more descriptive label \n",
    "    \n",
    "**<font color='blue'> _CLEANING & TRANSFORMATION ACTIVITIES_ </font>**  \n",
    "* `VEHICLE_ID` - impute after joining VEHICLE table\n",
    "* `SEX` - fill in blank values with \"unknown\" \n",
    "* `INJ_LEVEL` - impute missing values with most frequent value/mode\n",
    "* `INJ_LEVEL_DESC` - impute unknown values based on INJ_LEVEL\n",
    "* `SEATING_POSITION` - impute missing values with \"No known\"\n",
    "* `HELMET_BELT_WORN` - impute missing values with \"No known\" \n",
    "* `ROAD_USER_TYPE` - no missing values\n",
    "* `ROAD_USER_TYPE_DESC` - no missing values\n",
    "* `LICENCE_STATE`, - impute missing values with \"Not known\" and others with meaningful description\n",
    "* `PEDEST_MOVEMENT` - impute missing values with \"Not known\" and others with meaningful description\n",
    "* `POSTCODE` - drop column as 20% of them are non-sensible postcodes and missing values\n",
    "* `TAKEN_HOSPITAL`, - impute missing values with \"Not known\" and others with meaningful description\n",
    "* `EJECTED_CODE`: impute missing values with \"Not known\" and others with meaningful description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"explore_surface_cond\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:10px\"><strong> Dataset: </strong> <strongstyle=\"color:blue\"> surface_cond</strong> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Statistics : df_surface_cond\n",
    "df_surface_cond.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some data\n",
    "df_surface_cond.show(2,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing/null values : df_surface_cond\n",
    "df_surface_cond.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_surface_cond.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue records for each column\n",
    "df_surface_cond.select([countDistinct(c).alias(c) for c in df_surface_cond.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue values for each column\n",
    "[df_surface_cond.groupBy(c).count().sort(col(c)).show(truncate=False) for c in df_surface_cond.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicate\n",
    "df_surface_cond.groupBy(\"ACCIDENT_NO\", \"SURFACE_COND_DESC\").count().filter(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing values: SURFACE_COND & Surface Cond Desc\n",
    "df_surface_cond.groupBy(\"SURFACE_COND\",\"SURFACE_COND_DESC\").count().sort(col(\"SURFACE_COND\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what sequence numbers are assigned to surface condition unknown\n",
    "# to determine whether to transform this value\n",
    "df_surface_cond.filter(col('SURFACE_COND') == 9)\\\n",
    "            .groupBy('SURFACE_COND_SEQ').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check ACCIDENT_NO validity (foreign key)\n",
    "\n",
    "# check whether ACCIDENT_NO in df_surface_cond exists in df_accidents\n",
    "invalid_accident_id = df_surface_cond.join(df_accident, df_surface_cond.ACCIDENT_NO == df_accident.ACCIDENT_NO, how='left_anti')\\\n",
    "                            .select(col('ACCIDENT_NO'))\n",
    "\n",
    "# print the number of invalid ACCIDENT_NO\n",
    "print(\"Invalid ACCIDENT_NO in df_surface_cond  :\", invalid_accident_id.count())\n",
    "\n",
    "# view few invalid info\n",
    "invalid_accident_id.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'> _SUMMARY_ </font>**  \n",
    "* <strong> **Duplicate** </strong>: No duplicate records.\n",
    "* <strong> **Missing values/null** </strong>:   No missing values\n",
    "* <strong> **Outliers** </strong>:  \n",
    "* <strong> **Data Consistency/Validity** </strong>:  \n",
    "* <strong> **Other** </strong>:  \n",
    "    \n",
    "**<font color='blue'> _CLEANING & TRANSFORMATION ACTIVITIES_ </font>**  \n",
    "1-accident can have many surface conditions. In order to avoid duplicated accidents infomation, this dataset needs to be converted from long to wide where surface conditions form individual columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"explore_surface_cond\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:10px\"><strong> Dataset: </strong> <strongstyle=\"color:blue\"> subdca</strong> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Statistics :df_subdca\n",
    "df_subdca.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some data\n",
    "df_subdca.show(2,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  checking missing/null values : df_subdca\n",
    "df_subdca.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_subdca.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue records for each column\n",
    "df_subdca.select([countDistinct(c).alias(c) for c in df_subdca.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue values for each column\n",
    "[df_subdca.groupBy(c).count().sort(col(c)).show(truncate=False) for c in df_subdca.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicate\n",
    "df_subdca.groupBy(\"ACCIDENT_NO\", \"SUB_DCA_CODE\").count().filter(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing values: SUB_DCA_CODE & Sub Dca Code Desc\n",
    "df_subdca.groupBy(\"SUB_DCA_CODE\",\"SUB_DCA_CODE_DESC\").count().sort(col(\"SUB_DCA_CODE\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SUB_DCA_CODE` has multiple codes with \"Unknown\" values.  \n",
    "There are 121-rows of `SUB_DCA_CODE`. 1-accidents have multiple `SUB_DCA_CODE`. This can increase complexity to pivot the dataset from long to wider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check ACCIDENT_NO validity (foreign key)\n",
    "\n",
    "# check whether ACCIDENT_NO in df_subdca exists in df_accidents\n",
    "invalid_accident_id = df_subdca.join(df_accident, df_subdca.ACCIDENT_NO == df_accident.ACCIDENT_NO, how='left_anti')\\\n",
    "                            .select(col('ACCIDENT_NO'))\n",
    "\n",
    "# print the number of invalid ACCIDENT_NO\n",
    "print(\"Invalid ACCIDENT_NO in df_subdca  :\", invalid_accident_id.count())\n",
    "\n",
    "# view few invalid info\n",
    "invalid_accident_id.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'> _SUMMARY_ </font>**  \n",
    "* <strong> **Duplicate** </strong>: No duplicate records.\n",
    "* <strong> **Missing values/null** </strong>: `SUB_DCA_CODE` has multiple codes with \"Unknown\" values.\t \n",
    "* <strong> **Outliers** </strong>:  \n",
    "* <strong> **Data Consistency/Validity** </strong>:  \n",
    "* <strong> **Other** </strong>:  \n",
    "    \n",
    "**<font color='blue'> _CLEANING & TRANSFORMATION ACTIVITIES_ </font>**  \n",
    "1-accident can have  multiple `SUB_DCA_CODE`(up to 6). \n",
    "\n",
    "In order to avoid duplicated accidents infomation, this dataset needs to be converted from long to wide where each `SUB_DCA_CODE` form individual columns. This can be complex, where columns can take up to 120-columns. This dataset should further group and refine,if required.\n",
    "\n",
    "Since DCA_CODE is also available in accident dataset, this dataset SUBDCA will not be used for modelling. No further action is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"explore_vehicle\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:10px\"><strong> Dataset: </strong> <strongstyle=\"color:blue\"> Vehicle</strong> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Statistics - df_vehicle\n",
    "df_vehicle.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some data\n",
    "df_vehicle.show(2,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  checking missing/null values : df_vehicle\n",
    "df_vehicle.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_vehicle.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue records for each column\n",
    "df_vehicle.select([countDistinct(c).alias(c) for c in df_vehicle.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue values for each column\n",
    "[df_vehicle.groupBy(c).count().sort(col(\"count\").desc()).show(truncate=False) for c in df_vehicle.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicate\n",
    "df_vehicle.groupBy(\"ACCIDENT_NO\", \"VEHICLE_ID\").count().filter(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing values: ROAD_SURFACE_TYPE & ROAD_SURFACE_TYPE_DESC\n",
    "df_vehicle.groupBy(\"ROAD_SURFACE_TYPE\",\"ROAD_SURFACE_TYPE_DESC\").count().sort(col(\"ROAD_SURFACE_TYPE\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing values: VEHICLE_TYPE & Vehicle Type Desc\n",
    "df_vehicle.groupBy(\"VEHICLE_TYPE\",\"VEHICLE_TYPE_DESC\").count().sort(col(\"VEHICLE_TYPE\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pairing values: TRAFFIC_CONTROL & Traffic Control Desc\n",
    "df_vehicle.groupBy(\"TRAFFIC_CONTROL\",\"TRAFFIC_CONTROL_DESC\").count().sort(col(\"TRAFFIC_CONTROL\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check ACCIDENT_NO validity (foreign key)\n",
    "\n",
    "# check whether ACCIDENT_NO in df_vehicle exists in df_accidents\n",
    "invalid_accident_id = df_vehicle.join(df_accident, df_vehicle.ACCIDENT_NO == df_accident.ACCIDENT_NO, how='left_anti')\\\n",
    "                            .select(col('ACCIDENT_NO'))\n",
    "\n",
    "# print the number of invalid ACCIDENT_NO\n",
    "print(\"Invalid ACCIDENT_NO in df_vehicle  :\", invalid_accident_id.count())\n",
    "\n",
    "# view few invalid info\n",
    "invalid_accident_id.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'> _SUMMARY_ </font>**  \n",
    "* <strong> **Duplicate** </strong>: No duplicate records.\n",
    "* <strong> **Missing values/null** </strong>: Not all infomation is provided for Vehicle 2   \n",
    "* <strong> **Outliers** </strong>:  \n",
    "* <strong> **Data Consistency/Validity** </strong>:  \n",
    "* <strong> **Other** </strong>:  \n",
    "    Inputs on vehicle's related info (brands) are not consistently spelled.\n",
    "    \n",
    "**<font color='blue'> _CLEANING & TRANSFORMATION ACTIVITIES_ </font>**  \n",
    "- Provide descriptive data: \n",
    "    - `VEHICLE_DCA_CODE`, `LAMPS`, `INITIAL_IMPACT`, `CAUGHT_FIRE`, `FUEL_TYPE`, \n",
    "    - `LEVEL_OF_DAMAGE`, `TRAILER_TYPE`, `DRIVER_INTENT`,`VEHICLE_MOVEMENT`\n",
    "- Update Vehicle make to most common value where it is obvious (assumed) what the make should be\n",
    "- Reorder to align codes with description columns\n",
    "- Drop Columns we will not use for analysis/modelling due to lack of completeness in dataset\n",
    "- Sort data by `ACCIDENT_NO`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"data_transformation\"></a>\n",
    "<div style=\"background:rgba(0,80,80,0.2);padding:10px;border-radius:4px\"><h2>Cleaning & Transformation</h2>\n",
    "<hr/>\n",
    "This section focuses on cleaning and transforming relevant individual datasets, whereby the final output is a single cleaned dataset to be used for next phase of assignment. The activity will split into three parts: <br>\n",
    " <br> - <b> Part 1 </b> : Clean and Transform individual dataframe/dataset                \n",
    " <br> - <b> Part 2 </b> : Join all the clean dataframes into a joined dataframe    \n",
    " <br> - <b> Part 3 </b> : Perform exploration, cleaning & transformation on the joined dataframe.   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"transform_part1\"></a>\n",
    "<div style=\"background:rgba(0,80,80,0.2);padding:10px;border-radius:4px\"><h3>Part 1: Clean and Transform individual dataframe/dataset </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"transform_atmosp_cond\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:10px\"><strong> Dataset: </strong> <strongstyle=\"color:blue\"> atmospheric_cond </strong> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'> _CLEANING & TRANSFORMATION ACTIVITIES_ </font>**  \n",
    "* One accident has multiple atmospheric conditions - Pivot the table from long to wide, where each atmospheric conditions form individual columns by `ACCIDENT_NO`  \n",
    "* Sort by `ACCIDENT_NO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf_atmospheric_cond = df_atmospheric_cond.groupBy('ACCIDENT_NO').pivot('ATMOSPH_COND_DESC').count()\\\n",
    "            .withColumn('ATMOSPH_CLEAR', when(col('Clear') == 1, \"Yes\").otherwise('No'))\\\n",
    "            .withColumn('ATMOSPH_RAINING', when(col('Raining') == 1, \"Yes\").otherwise('No'))\\\n",
    "            .withColumn('ATMOSPH_SNOWING', when(col('Snowing') == 1, \"Yes\").otherwise('No'))\\\n",
    "            .withColumn('ATMOSPH_FOG', when(col('Fog') == 1, \"Yes\").otherwise('No'))\\\n",
    "            .withColumn('ATMOSPH_SMOKE', when(col('Smoke') == 1, \"Yes\").otherwise('No'))\\\n",
    "            .withColumn('ATMOSPH_DUST', when(col('Dust') == 1, \"Yes\").otherwise('No'))\\\n",
    "            .withColumn('ATMOSPH_STRONG_WINDS', when(col('Strong winds') == 1, \"Yes\").otherwise('No'))\\\n",
    "            .withColumn('ATMOSPH_UNKNOWN', when(col('Not known') == 1, \"Yes\").otherwise('No'))\\\n",
    "            .drop('Clear', 'Raining','Snowing','Fog','Smoke','Dust','Strong winds', 'Not known')\\\n",
    "            .sort(\"ACCIDENT_NO\")\\\n",
    "            .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of records & number of columns after transformation\n",
    "ndf_atmospheric_cond_count = ndf_atmospheric_cond.count()\n",
    "ndf_atmospheric_cond_len = len(ndf_atmospheric_cond.columns)\n",
    "print(\"Number of Records: \", ndf_atmospheric_cond_count, \" Number of columns: \", ndf_atmospheric_cond_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result\n",
    "ndf_atmospheric_cond.show(2,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"transform_surface_cond\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:10px\"><strong> Dataset: </strong> <strongstyle=\"color:blue\"> surface_cond</strong> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'> _CLEANING & TRANSFORMATION ACTIVITIES_ </font>**  \n",
    "- Remove Surface_Condition Columns and Pivot Sequence data to give all values for an accident on a single row as per metadata information \n",
    "- Sort by `ACCIDENT_NO` to allow any joining operation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One accident has multiple surface conditions\n",
    "# Pivot the table from long to wide\n",
    "ndf_surface_cond = df_surface_cond.groupBy('ACCIDENT_NO').pivot('SURFACE_COND_DESC').count()\\\n",
    "            .withColumn('SURFACE_DRY', when(col('Dry') == 1, \"Yes\").otherwise('No'))\\\n",
    "            .withColumn('SURFACE_WET', when(col('Wet') == 1, \"Yes\").otherwise('No'))\\\n",
    "            .withColumn('SURFACE_MUDDY', when(col('Muddy') == 1, \"Yes\").otherwise('No'))\\\n",
    "            .withColumn('SURFACE_SNOWY', when(col('Snowy') == 1, \"Yes\").otherwise('No'))\\\n",
    "            .withColumn('SURFACE_ICY', when(col('Icy') == 1, \"Yes\").otherwise('No'))\\\n",
    "            .withColumn('SURFACE_UNKNOWN', when(col('Unknown') == 1, \"Yes\").otherwise('No'))\\\n",
    "            .drop('Dry', 'Wet','Muddy','Snowy','Icy','Unknown').sort(\"ACCIDENT_NO\")\\\n",
    "            .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of records & number of columns after transformation\n",
    "ndf_surface_cond_count = ndf_surface_cond.count()\n",
    "ndf_surface_cond_len = len(ndf_surface_cond.columns)\n",
    "print(\"Number of Records: \", ndf_surface_cond_count, \" Number of columns: \", ndf_surface_cond_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result\n",
    "ndf_surface_cond.show(2,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"transform_person\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:10px\"><strong> Dataset: </strong> <strongstyle=\"color:blue\"> person </strong> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'> _CLEANING & TRANSFORMATION ACTIVITIES_ </font>**  \n",
    "* remove/trim character space for all records\n",
    "* fill missing values of `AGE` with 'average_age'\n",
    "* fill missing values of `INJ_LEVEL` with 'mode_injury_level'\n",
    "* `AGE_GROUP` - reclassify age group to sensible age range\n",
    "* `LICENCE_STATE`, - reclassify state to smaller ranges, fill missing value  with \"Not known\" and others with meaningful description\n",
    "* `SEX` - fill in blank values with \"unknown\" and others with meaningful description   \n",
    "* `HELMET_BELT_WORN` - fill missing value  with \"Not known\" and others with meaningful description\n",
    "* `SEATING_POSITION` - fill missing value  with \"Not known\" and others with meaningful description \n",
    "* `EJECTED_CODE` - fill missing value  with \"Not known\" and others with meaningful description\n",
    "* `TAKEN_HOSPITAL` - fill missing value  with \"Not known\" and others with meaningful description\n",
    "* `Inj Level Desc` - fill missing value  with \"Not known\" and others with meaningful description \n",
    "* `PEDEST_MOVEMENT` - fill missing value  with \"Not known\" and others with meaningful description \n",
    "* Select only columns requires for next part of processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set mode value for INJ_LEVEL (manage missing values)\n",
    "mode_inj_lvl = df_person.groupBy(\"INJ_LEVEL\").agg(F.count(col(\"INJ_LEVEL\")).alias('count_INJ_LEVEL')).sort(col(\"INJ_LEVEL\"), ascending=False).first()['INJ_LEVEL']\n",
    "print(mode_inj_lvl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set average age (manage missing values)\n",
    "# note, the average age by gender is between 37 or 38 for Female and Male respectively, where it belongs to the same Age_Group. \n",
    "# For simplicity, average_age is used.\n",
    "avg_age = df_person.agg(F.round(F.avg(col(\"AGE\"))).cast('integer').alias('average_age')).first()['average_age']\n",
    "print(avg_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform all data cleaning and transformation as specify above\n",
    "ndf_person = df_person.select([trim(col(c)).alias(c) for c in df_person.columns])\\\n",
    "                    .withColumn(\"AGE\",\n",
    "                         when(col('AGE').isNull(), avg_age)\\\n",
    "                        .otherwise(col('AGE')))\\\n",
    "                    .withColumn(\"INJ_LEVEL\",\n",
    "                         when(col(\"INJ_LEVEL\").isNull(), mode_inj_lvl)\\\n",
    "                        .otherwise(col(\"INJ_LEVEL\")))\\\n",
    "                    .withColumn(\"AGE_GROUP\",\n",
    "                            when(col(\"AGE\").between(0,4), \"0-4\")\\\n",
    "                           .when(col(\"AGE\").between(5,17), \"5-17\")\\\n",
    "                           .when(col(\"AGE\").between(18,24), \"18-24\")\\\n",
    "                           .when(col(\"AGE\").between(25,34), \"25-34\")\\\n",
    "                           .when(col(\"AGE\").between(35,44), \"35-44\")\\\n",
    "                           .when(col(\"AGE\").between(45,54), \"45-54\")\\\n",
    "                           .when(col(\"AGE\").between(55,64), \"55-64\")\\\n",
    "                           .when(col(\"AGE\").between(65,69), \"65-69\")\\\n",
    "                           .when(col(\"AGE\") > 69, \"70+\")\\\n",
    "                           .otherwise(col(\"AGE\")))\\\n",
    "                    .withColumn(\"LICENCE_STATE\",\n",
    "                         when(col(\"LICENCE_STATE\").isin(\"A\",\"D\",\"N\",\"Q\",\"S\",\"T\",\"W\"), \"Interstate\")\\\n",
    "                        .when(col(\"LICENCE_STATE\") == \"B\", \"Commonwealth\")\\\n",
    "                        .when(col(\"LICENCE_STATE\") == \"O\", \"Overseas\")\\\n",
    "                        .when(col(\"LICENCE_STATE\") == \"V\", \"Victoria\")\\\n",
    "                        .otherwise(\"Not known\"))\\\n",
    "                    .withColumn(\"SEX\", \n",
    "                         when(col('SEX') == 'F',\"Female\")\\\n",
    "                        .when(col('SEX') == 'M',\"Male\")\\\n",
    "                        .otherwise(\"Not known\"))\\\n",
    "                    .withColumn(\"HELMET_BELT_WORN_DESC\", \n",
    "                         when(col('HELMET_BELT_WORN') == 1,\"Seatbelt worn\")\\\n",
    "                        .when(col('HELMET_BELT_WORN') == 2,\"Seatbelt not worn\")\\\n",
    "                        .when(col('HELMET_BELT_WORN') == 3,\"Child restraint worn\")\\\n",
    "                        .when(col('HELMET_BELT_WORN') == 4,\"Child restraint not worn\")\\\n",
    "                        .when(col('HELMET_BELT_WORN') == 5,\"Seatbelt/restraint not fitted\")\\\n",
    "                        .when(col('HELMET_BELT_WORN') == 6,\"Crash helmet worn\")\\\n",
    "                        .when(col('HELMET_BELT_WORN') == 7,\"Crash helmet not worn\")\\\n",
    "                        .when(col('HELMET_BELT_WORN') == 8,\"Not appropriate\")\\\n",
    "                        .otherwise(\"Not known\"))\\\n",
    "                    .withColumn(\"SEATING_POSITION_DESC\", \n",
    "                         when(col('SEATING_POSITION') == 'CF',\"Centre-front\")\\\n",
    "                        .when(col('SEATING_POSITION') == 'CR',\"Centre-rear\")\\\n",
    "                        .when(col('SEATING_POSITION') == 'D' ,\"Driver or rider\")\\\n",
    "                        .when(col('SEATING_POSITION') == 'LF',\"Left-front\")\\\n",
    "                        .when(col('SEATING_POSITION') == 'LR',\"Left-rear\")\\\n",
    "                        .when(col('SEATING_POSITION') == 'NA',\"Not applicable\")\\\n",
    "                        .when(col('SEATING_POSITION') == 'NK',\"Not known\")\\\n",
    "                        .when(col('SEATING_POSITION') == 'OR',\"Other-rear\")\\\n",
    "                        .when(col('SEATING_POSITION') == 'PL',\"Pillion passenger\")\\\n",
    "                        .when(col('SEATING_POSITION') == 'PS',\"Motor-cycle side car passenger\")\\\n",
    "                        .when(col('SEATING_POSITION') == 'RR',\"Right-rear\")\\\n",
    "                        .otherwise(\"Not known\"))\\\n",
    "                    .withColumn(\"EJECTED_CODE\", \n",
    "                         when(col(\"EJECTED_CODE\") == \"0\", \"Not applicable\")\\\n",
    "                        .when(col(\"EJECTED_CODE\") == \"1\", \"Total ejected\")\\\n",
    "                        .when(col(\"EJECTED_CODE\") == \"2\", \"Partially ejected\")\\\n",
    "                        .when(col(\"EJECTED_CODE\") == \"3\", \"Partial ejection involving extraction\")\\\n",
    "                        .otherwise(\"Not known\"))\\\n",
    "                    .withColumn(\"TAKEN_HOSPITAL\",\n",
    "                         when(col(\"TAKEN_HOSPITAL\") == \"Y\",\"Yes\")\\\n",
    "                        .when(col(\"TAKEN_HOSPITAL\") == \"N\", \"No\")\\\n",
    "                        .otherwise(\"Not known\"))\\\n",
    "                    .withColumn(\"INJ_LEVEL_DESC\", \n",
    "                         when(col('INJ_LEVEL') == \"4\", \"4-Not Injured\")\\\n",
    "                        .when(col('INJ_LEVEL') == \"3\", \"3-Other Injury\")\\\n",
    "                        .when(col('INJ_LEVEL') == \"2\", \"2-Serious Injury\")\\\n",
    "                        .when(col('INJ_LEVEL') == \"1\", \"1-Fatality\")\\\n",
    "                        .otherwise(col(\"INJ_LEVEL_DESC\")))\\\n",
    "                    .withColumn(\"PEDEST_MOVEMENT_DESC\",\n",
    "                         when(col(\"PEDEST_MOVEMENT\") == \"0\", \"Not applicable\")\\\n",
    "                        .when(col(\"PEDEST_MOVEMENT\") == \"1\", \"Crossing carriageway\")\\\n",
    "                        .when(col(\"PEDEST_MOVEMENT\") == \"2\", \"Working/playing/lying or standing on carriageway\")\\\n",
    "                        .when(col(\"PEDEST_MOVEMENT\") == \"3\", \"Walking on carriageway with traffic\")\\\n",
    "                        .when(col(\"PEDEST_MOVEMENT\") == \"4\", \"Walking on carriageway against traffic\")\\\n",
    "                        .when(col(\"PEDEST_MOVEMENT\") == \"5\", \"Pushing or working on vehicle\")\\\n",
    "                        .when(col(\"PEDEST_MOVEMENT\") == \"6\", \"Walking to/from or boarding tram\")\\\n",
    "                        .when(col(\"PEDEST_MOVEMENT\") == \"7\", \"Walking to/from or boarding other vehicle\")\\\n",
    "                        .when(col(\"PEDEST_MOVEMENT\") == \"8\", \"Not on carriageway\")\\\n",
    "                        .otherwise(\"Not known\"))\\\n",
    "                    .select(\"ACCIDENT_NO\",\"PERSON_ID\", \"VEHICLE_ID\",\"SEX\",\"AGE_GROUP\",\"LICENCE_STATE\",\n",
    "                            \"INJ_LEVEL_DESC\",\"TAKEN_HOSPITAL\",\"EJECTED_CODE\",\n",
    "                            \"ROAD_USER_TYPE_DESC\",\"SEATING_POSITION_DESC\",\"HELMET_BELT_WORN_DESC\",\n",
    "                            \"PEDEST_MOVEMENT_DESC\")\\\n",
    "                    .cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of records & number of columns after transformation\n",
    "ndf_person_count = ndf_person.count()\n",
    "ndf_person_len = len(ndf_person.columns)\n",
    "print(\"Number of Records: \", ndf_person_count, \" Number of columns: \", ndf_person_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result\n",
    "ndf_person.show(2,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"transform_node\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:10px\"><strong> Dataset: </strong> <strongstyle=\"color:blue\"> node </strong> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'> _CLEANING & TRANSFORMATION ACTIVITIES_ </font>**  \n",
    "* `LGA_NAME_ALL`, `LGA_NAME`, `REGION NAME`, `DEG_URBAN_NAME` - fill empty values and aligning the standard description.\n",
    "* Remove duplicate entries where `ACCIDENT_NO` and `NODE_ID` are the same.\n",
    "* Removing duplicates have also removed the missing values entries.\n",
    "* Update `NODE_TYPE`  with more descriptive labels.\n",
    "* Select only columns requires for next part of processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special character for LGA_NAME\n",
    "ndf_node_temp1 = df_node.withColumn(\"LGA_NAME\", F.regexp_replace(col(\"LGA_NAME\"), \"[\\(.*\\)]\", \"\"))\\\n",
    "    .withColumn(\"LGA_NAME\", F.regexp_replace(col(\"LGA_NAME_ALL\"), \"[\\(.*\\)]\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Views from Dataframes\n",
    "ndf_node_temp1.createOrReplaceTempView(\"sql_node\")\n",
    "ndf_node_temp1.createOrReplaceTempView(\"sql_node_temp\")\n",
    "\n",
    "#### update the description of REGION_NAME, LGA_NAME, LGA_NAME_ALL based on postcode\n",
    "# assume the most entered data are the correct values of mapping\n",
    "ndf_node_temp2 = spark.sql('''\n",
    "    SELECT n.ACCIDENT_NO, n.NODE_ID, n.NODE_TYPE, n.VICGRID94_X, n.VICGRID94_Y,\n",
    "            p.LGA_NAME, p.LGA_NAME_ALL, p.REGION_NAME, p.DEG_URBAN_NAME, \n",
    "            n.Lat, n.Long, n.POSTCODE_NO\n",
    "    FROM sql_node_temp n INNER JOIN \n",
    "        (SELECT POSTCODE_NO, REGION_NAME, LGA_NAME, LGA_NAME_ALL, DEG_URBAN_NAME\n",
    "        FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY POSTCODE_NO ORDER BY Rank) as row_num\n",
    "            FROM  (SELECT POSTCODE_NO, REGION_NAME, LGA_NAME, LGA_NAME_ALL, DEG_URBAN_NAME,\n",
    "                    count(*), \n",
    "                    rank() OVER(PARTITION BY POSTCODE_NO ORDER BY count(*) DESC) as Rank                    \n",
    "                  FROM sql_node\n",
    "                  GROUP BY POSTCODE_NO, REGION_NAME, LGA_NAME, LGA_NAME_ALL, DEG_URBAN_NAME\n",
    "                  ORDER BY POSTCODE_NO)\n",
    "            WHERE Rank = 1)\n",
    "    WHERE row_num = 1) p\n",
    "    ON n.POSTCODE_NO =  p.POSTCODE_NO\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of records before & after description is updated\n",
    "# the number of records should be the same\n",
    "print(\"Number of records - BEFORE: \", df_shape_initial.filter((col('DATAFRAME') == \"df_node\") & (col(\"SHAPE\") == \"Number of Records\")).first()[\"INITIAL RESULTS\"], \"| AFTER: \", ndf_node_temp2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the result of translation\n",
    "ndf_node_temp2.show(2,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate rows assuming the first row is the correct entry\n",
    "# Add description for Node_Type\n",
    "ndf_node = ndf_node_temp2.dropDuplicates([\"NODE_ID\",\"ACCIDENT_NO\"])\\\n",
    "                    .withColumn(\"NODE_TYPE_DESC\", \n",
    "                                when(col('NODE_TYPE') == 'I',\"Intersection\")\\\n",
    "                                .when(col('NODE_TYPE') == 'N',\"Non-intersection\")\\\n",
    "                                .when(col('NODE_TYPE') == 'O',\"Off-road\")\\\n",
    "                                .when(col('NODE_TYPE') == 'U',\"Unknown\")\\\n",
    "                                .otherwise(\"Unknown\"))\\\n",
    "                    .select(\"ACCIDENT_NO\",\"NODE_ID\",\"NODE_TYPE_DESC\",\"LGA_NAME\",\"REGION_NAME\",\"DEG_URBAN_NAME\")\\\n",
    "                    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of records & number of columns after transformation\n",
    "ndf_node_count = ndf_node.count()\n",
    "ndf_node_len = len(ndf_node.columns)\n",
    "print(\"Number of Records: \", ndf_node_count, \" Number of columns: \", ndf_node_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result\n",
    "ndf_node.show(2,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicate records after transformation\n",
    "ndf_node.groupby(\"NODE_ID\",\"ACCIDENT_NO\").agg(count(col(\"ACCIDENT_NO\")).alias('num_of_accidents')).sort(col(\"NODE_ID\"))\\\n",
    "        .filter(col('num_of_accidents') > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"transform_accident\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:10px\"><strong> Dataset: </strong> <strongstyle=\"color:blue\"> accident</strong> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'> _CLEANING & TRANSFORMATION ACTIVITIES_ </font>**   \n",
    "- Remove spaces from all fields for all columns  \n",
    "- Update `POLICE_ATTEND` to be more prescritive, based on meta-data's reference.  \n",
    "- Add `SEVERITY_DESC` to presribe `SEVERITY`.   \n",
    "- Add `SPEED_ZONE_DESC` to presribe `SPEED_ZONE`.  \n",
    "- Create `ACCIDENT_HOUR_GROUP` based on `ACCIDENTTIME` (MORNING PEAK (7am-9am), EVENING PEAK (4pm-7pm), OFF-PEAK). \n",
    "- Select only columns requires for next part of processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform above cleaning and transformationa activities\n",
    "ndf_accident = df_accident.select([trim(col(c)).alias(c) for c in df_accident.columns])\\\n",
    "                        .withColumn(\"POLICE_ATTEND\", \n",
    "                             when(col('POLICE_ATTEND') == 1,\"Yes\")\\\n",
    "                            .when(col('POLICE_ATTEND') == 2,\"No\")\\\n",
    "                            .when(col('POLICE_ATTEND') == 9,\"Not known\")\\\n",
    "                            .otherwise(\"Not known\"))\\\n",
    "                        .withColumn(\"SEVERITY_DESC\", \n",
    "                             when(col('SEVERITY') == 1,\"1-Fatal accident\")\\\n",
    "                            .when(col('SEVERITY') == 2,\"2-Serious injury accident\")\\\n",
    "                            .when(col('SEVERITY') == 3,\"3-Other injury accident\")\\\n",
    "                            .when(col('SEVERITY') == 4,\"4-Non injury accident\")\\\n",
    "                            .otherwise(\"4 - Non injury accident\"))\\\n",
    "                        .withColumn(\"SPEED_ZONE_DESC\", \n",
    "                             when(col('SPEED_ZONE') == '030',\"30-40 km/hr\")\\\n",
    "                            .when(col('SPEED_ZONE') == '040',\"30-40 km/hr\")\\\n",
    "                            .when(col('SPEED_ZONE') == '050',\"50 km/hr\")\\\n",
    "                            .when(col('SPEED_ZONE') == '060',\"60 km/hr\")\\\n",
    "                            .when(col('SPEED_ZONE') == '070',\"70-75 km/hr\")\\\n",
    "                            .when(col('SPEED_ZONE') == '075',\"70-75 km/hr\")\\\n",
    "                            .when(col('SPEED_ZONE') == '080',\"80-90 km/hr\")\\\n",
    "                            .when(col('SPEED_ZONE') == '090',\"80-90 km/hr\")\\\n",
    "                            .when(col('SPEED_ZONE') == '100',\"100-110 km/hr\")\\\n",
    "                            .when(col('SPEED_ZONE') == '110',\"100-110 km/hr\")\\\n",
    "                            .when(col('SPEED_ZONE') == '777',\"Other/Not Known\")\\\n",
    "                            .when(col('SPEED_ZONE') == '888',\"Other/Not Known\")\\\n",
    "                            .when(col('SPEED_ZONE') == '999',\"Other/Not Known\")\\\n",
    "                            .otherwise(\"Other/Not Known\"))\\\n",
    "                        .withColumn(\"ACCIDENT_HOUR\", substring(col(\"ACCIDENTTIME\"),1,2).cast(IntegerType()))\\\n",
    "                        .withColumn(\"ACCIDENT_MONTH\", regexp_extract(col(\"ACCIDENTDATE\"),r'\\/(\\d*)\\/',1).cast(IntegerType()))\\\n",
    "                        .withColumn(\"ACCIDENT_HOUR_GROUP\", \n",
    "                                    when((( col(\"ACCIDENT_HOUR\") >= 7) & (col(\"ACCIDENT_HOUR\") < 9)),\"MORNING PEAK\")\\\n",
    "                                    .when((( col(\"ACCIDENT_HOUR\") >= 16) & (col(\"ACCIDENT_HOUR\") < 19)),\"EVENING PEAK\")\\\n",
    "                                    .otherwise(\"OFF-PEAK\"))\\\n",
    "                        .select(\"ACCIDENT_NO\",\"ACCIDENTDATE\",\"ACCIDENT_MONTH\",\"ACCIDENT_HOUR_GROUP\",\"DAY_OF_WEEK_DESC\",\n",
    "                                \"SEVERITY_DESC\", \"ACCIDENT_TYPE_DESC\",\"DCA_DESC\",\"LIGHT_CONDITION_DESC\",\"ROAD_GEOMETRY_DESC\",\n",
    "                                \"POLICE_ATTEND\",\"SPEED_ZONE_DESC\",\"NODE_ID\")\\\n",
    "                        .cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of records & number of columns after transformation\n",
    "ndf_accident_count = ndf_accident.count()\n",
    "ndf_accident_len = len(ndf_accident.columns)\n",
    "print(\"Number of Records: \", ndf_accident_count, \" Number of columns: \", ndf_accident_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result\n",
    "ndf_accident.show(2,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"transform_vehicle\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:10px\"><strong> Dataset: </strong> <strongstyle=\"color:blue\"> vehicle </strong> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Provide descriptive data for matched code fields where available from https://data.vicroads.vic.gov.au/Metadata/Crash%20Stats%20-%20Data%20Extract%20-%20Open%20Data.html\n",
    "    - `VEHICLE_DCA_CODE`, `LAMPS`, `INITIAL_IMPACT`, `CAUGHT_FIRE`, `FUEL_TYPE`, \n",
    "    - `LEVEL_OF_DAMAGE`, `TRAILER_TYPE`, `DRIVER_INTENT`,`VEHICLE_MOVEMENT`\n",
    "- Create `VEHICLE_CHANGED_DIRECTION` from the INITIAL_DIRECTION/FINAL_DIRECTION columns\n",
    "- Group `REG_STATE` into `Victoria` or `Other`\n",
    "- Created a grouped `VEHICLE_YEAR_MANUF` for vehicles <5 Yeas old, 5-10 years old, 10-20 Years old, 20-30 Years old, 30+Years old\n",
    "- Correct Vehicle Make values to be a  most common value where it is obvious (assumed) what the make should be\n",
    "- Reorder to align codes with description columns\n",
    "- Rename columns ( remove spaces and capitalise)\n",
    "- Calculate `VEHICLE_CHANGED_DIRECTION`\n",
    "- Drop Columns we will not use for analysis/modelling due to lack of completeness in data set\n",
    "    - INITIAL_DIRECTION, TOWED_AWAY_FLAG, VEHICLE_MODEL, CONSTRUCTION_TYPE, VEHICLE_WEIGHT, TARE_WEIGHT, CARRY_CAPACITY, CUBIC_CAPCITY, FINAL_DIRECTION\n",
    "- Sort data by `ACCIDENT_NO`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup as per above notes\n",
    "\n",
    "ndf_vehicle = df_vehicle.withColumn(\"REG_STATE_GRP\", \n",
    "                                    when(col('REG_STATE').isin(\"A\",\"D\",\"N\",\"Q\",\"S\",\"T\",\"W\"), \"Interstate\")\\\n",
    "                                    .when(col('REG_STATE') == \"B\", \"Commonwealth\")\\\n",
    "                                    .when(col('REG_STATE') == \"O\",\"Overseas\")\\\n",
    "                                    .when(col('REG_STATE') == \"V\",\"Victoria\")\\\n",
    "                                    .when(col('REG_STATE') == \"Z\",\"Not known\")\\\n",
    "                                    .otherwise(\"Not known\"))\\\n",
    "                        .withColumn(\"VEHICLE_DCA_DESC\", \n",
    "                                 when(col('VEHICLE_DCA_CODE') == 1,\"Vehicle 1\")\\\n",
    "                                .when(col('VEHICLE_DCA_CODE') == 2,\"Vehicle 2\")\\\n",
    "                                .when(col('VEHICLE_DCA_CODE') == 3,\"Not known which vehicle was number 1\")\\\n",
    "                                .when(col('VEHICLE_DCA_CODE') == 8,\"Not involved in initial event\")\\\n",
    "                                .otherwise(\"Not known\"))\\\n",
    "                        .withColumn(\"VEHICLE_YEAR_MANUF\", when(col('VEHICLE_YEAR_MANUF') == 0,\"Not known\")\\\n",
    "                                    .when(col('VEHICLE_YEAR_MANUF') > 2020 ,\"Not known\")\\\n",
    "                                   .otherwise(df_vehicle[\"VEHICLE_YEAR_MANUF\"]))\\\n",
    "                        .withColumn(\"LAMPS_ON\", \n",
    "                                     when(col('LAMPS') == 0,\"Not Applicable\")\\\n",
    "                                    .when(col(\"LAMPS\") == 1,\"Yes\")\\\n",
    "                                    .when(col(\"LAMPS\") == 2,\"No\")\\\n",
    "                                    .otherwise(\"Not known\"))\\\n",
    "                        .withColumn(\"INITIAL_IMPACT_DESC\", \n",
    "                                     when(col('INITIAL_IMPACT') == 0,\"Towed Unit\")\\\n",
    "                                    .when(col(\"INITIAL_IMPACT\") == 1,\"Right front corner\")\\\n",
    "                                    .when(col(\"INITIAL_IMPACT\") == 2,\"Right side forwards\")\\\n",
    "                                    .when(col(\"INITIAL_IMPACT\") == 3,\"Right side rearwards\")\\\n",
    "                                    .when(col(\"INITIAL_IMPACT\") == 4,\"Right rear corner\")\\\n",
    "                                    .when(col(\"INITIAL_IMPACT\") == 6,\"Left front corner\")\\\n",
    "                                    .when(col(\"INITIAL_IMPACT\") == 6,\"Left side forwards\")\\\n",
    "                                    .when(col(\"INITIAL_IMPACT\") == 7,\"Left side rearwards\")\\\n",
    "                                    .when(col(\"INITIAL_IMPACT\") == 8,\"Left rear corner\")\\\n",
    "                                    .when(col(\"INITIAL_IMPACT\") == 9,\"Not Known/Not Applicable\")\\\n",
    "                                    .when(col(\"INITIAL_IMPACT\") == \"F\",\"Front\")\\\n",
    "                                    .when(col(\"INITIAL_IMPACT\") == \"N\",\"None\")\\\n",
    "                                    .when(col(\"INITIAL_IMPACT\") == \"R\",\"Sidecar\")\\\n",
    "                                    .when(col(\"INITIAL_IMPACT\") == \"T\",\"Top/roof\")\\\n",
    "                                    .when(col(\"INITIAL_IMPACT\") == \"U\",\"Undercarriage\")\\\n",
    "                                    .otherwise(\"Not known\"))\\\n",
    "                        .withColumn(\"CAUGHT_FIRE\", \n",
    "                                     when(col('CAUGHT_FIRE') == 0,\"Not Applicable\")\\\n",
    "                                    .when(col(\"CAUGHT_FIRE\") == 1,\"Yes\")\\\n",
    "                                    .when(col(\"CAUGHT_FIRE\") == 2,\"No\")\\\n",
    "                                    .otherwise(\"Not known\"))\\\n",
    "                        .withColumn(\"FUEL_TYPE_DESC\", \n",
    "                                     when(col(\"FUEL_TYPE\") == \"D\",\"Diesel\")\\\n",
    "                                    .when(col(\"FUEL_TYPE\") == \"E\", \"Electric\")\\\n",
    "                                    .when(col(\"FUEL_TYPE\") == \"G\", \"Gas\")\\\n",
    "                                    .when(col(\"FUEL_TYPE\") == \"M\", \"Multi\")\\\n",
    "                                    .when(col(\"FUEL_TYPE\") == \"P\", \"Petrol\")\\\n",
    "                                    .when(col(\"FUEL_TYPE\") == \"R\", \"Rotary\")\\\n",
    "                                    .otherwise(\"Unknown\"))\\\n",
    "                        .withColumn(\"LEVEL_OF_DAMAGE_DESC\", \n",
    "                                     when(col(\"LEVEL_OF_DAMAGE\") == 1,\"Minor\")\\\n",
    "                                    .when(col(\"LEVEL_OF_DAMAGE\") == 2,\"Moderate (driveable vehicle)\")\\\n",
    "                                    .when(col(\"LEVEL_OF_DAMAGE\") == 3,\"Moderate (unit towed away)\")\\\n",
    "                                    .when(col(\"LEVEL_OF_DAMAGE\") == 4,\"Major (unit towed away)\")\\\n",
    "                                    .when(col(\"LEVEL_OF_DAMAGE\") == 5,\"Extensive (unrepairable)\")\\\n",
    "                                    .when(col(\"LEVEL_OF_DAMAGE\") == 6,\"Nil damage\")\\\n",
    "                                    .otherwise(\"Not known\"))\\\n",
    "                        .withColumn(\"TRAILER_TYPE_DESC\", \n",
    "                                     when(col(\"TRAILER_TYPE\") == \"A\",\"Caravan\")\\\n",
    "                                    .when(col(\"TRAILER_TYPE\") == \"B\",\"Trailer (general)\")\\\n",
    "                                    .when(col(\"TRAILER_TYPE\") == \"C\",\"Trailer (boat)\")\\\n",
    "                                    .when(col(\"TRAILER_TYPE\") == \"D\",\"Horse float\")\\\n",
    "                                    .when(col(\"TRAILER_TYPE\") == \"E\",\"Machinery\")\\\n",
    "                                    .when(col(\"TRAILER_TYPE\") == \"F\",\"Farm/agricultural equipment\")\\\n",
    "                                    .when(col(\"TRAILER_TYPE\") == \"G\",\"Not known what is being towed\")\\\n",
    "                                    .when(col(\"TRAILER_TYPE\") == \"H\",\"Not Applicable\")\\\n",
    "                                    .when(col(\"TRAILER_TYPE\") == \"I\",\"Trailer (exempt)\")\\\n",
    "                                    .when(col(\"TRAILER_TYPE\") == \"J\",\"Semi Trailer\")\\\n",
    "                                    .when(col(\"TRAILER_TYPE\") == \"K\",\"Pig Trailer\")\\\n",
    "                                    .when(col(\"TRAILER_TYPE\") == \"L\",\"Dog Trailer\")\\\n",
    "                                    .otherwise(\"Not known\"))\\\n",
    "                        .withColumn(\"DRIVER_INTENT_DESC\", \n",
    "                                     when(col(\"DRIVER_INTENT\") == \"01\",\"Going straight ahead\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"02\",\"Turning right\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"03\",\"Turning left\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"04\",\"Leaving a driveway\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"05\",\"U turning \")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"06\",\"Changing lanes\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"07\",\"Overtaking\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"08\",\"Merging\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"09\",\"Reversing\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"10\",\"Parking or unparking\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"11\",\"Parked legally\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"12\",\"Parked illegally\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"13\",\"Stationary accident\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"14\",\"Stationary broken down\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"15\",\"Other stationary\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"16\",\"Avoiding animals\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"17\",\"Slow/stopping\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"18\",\"Out of control\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"19\",\"Wrong way\")\\\n",
    "                                    .when(col(\"DRIVER_INTENT\") == \"99\",\"Not known\")\\\n",
    "                                    .otherwise(\"Not known\"))\\\n",
    "                        .withColumn(\"VEHICLE_MOVEMENT_DESC\", \n",
    "                                     when(col(\"VEHICLE_MOVEMENT\") == \"01\",\"Going straight ahead\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"02\",\"Turning right\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"03\",\"Turning left\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"04\",\"Leaving a driveway\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"05\",\"U turning \")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"06\",\"Changing lanes\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"07\",\"Overtaking\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"08\",\"Merging\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"09\",\"Reversing\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"10\",\"Parking or unparking\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"11\",\"Parked legally\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"12\",\"Parked illegally\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"13\",\"Stationary accident\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"14\",\"Stationary broken down\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"15\",\"Other stationary\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"16\",\"Avoiding animals\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"17\",\"Slow/stopping\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"18\",\"Out of control\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"19\",\"Wrong way\")\\\n",
    "                                    .when(col(\"VEHICLE_MOVEMENT\") == \"99\",\"Not known\")\\\n",
    "                                    .otherwise(\"Not known\"))\\\n",
    "                        .withColumn(\"VEHICLE_MAKE\", \n",
    "                                     when(col(\"VEHICLE_MAKE\") == \"B.M.W.\",\"B M W\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"B.M.\",\"B M W\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"CATERP\",\"CATPLR\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"CF MOT\",\"CFMO\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"CHEVRO\",\"CHEV\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"CHRYSL\",\"CHRYS\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"CITROE\",\"CITRN\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"H DA\",\"H DAV\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"HSQV\",\"HSQVRN\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"HYND\",\"HYNDAI\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"HYU\",\"HYNDAI\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"HYUNDA\",\"HYNDAI\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"HYUNDI\",\"HYNDAI\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"KAWS\",\"KAWASA\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"KAWASK\",\"KAWASA\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"KENWOR\",\"KENWTH\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"L RO\",\"L ROV\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"LROVER\",\"L ROV\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"MER\",\"MERC B\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"MERCBZ\",\"MERC B\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"MERCED\",\"MERC B\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"MISTUB\",\"MITSUB\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"NIS\",\"NISSAN\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"PEU\",\"PEUG\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"PEUGEO\",\"PEUG\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"PGO\",\"PEUG\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"RENAUL\",\"REN\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"TRIUMP\",\"TRIUM\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"UNK\",\"UNKN\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"UNKW\",\"UNKN\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"UNSPEC\",\"UNKN\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"VOLKSW\",\"VOLKS\")\\\n",
    "                                    .when(col(\"VEHICLE_MAKE\") == \"VW\",\"VOLKS\")\n",
    "                                    .otherwise(col(\"VEHICLE_MAKE\")))\\\n",
    "                    .withColumn(\"VM_IND\", when(col(\"VEHICLE_MAKE\") == 'UNKN  ',0).otherwise(1))\\\n",
    "                    .withColumn(\"VM_IND\", when(col(\"VEHICLE_MAKE\") == '      ',0).otherwise(1))\\\n",
    "                    .withColumn(\"VMCNT\",F.sum(\"VM_IND\").over(Window.partitionBy(\"VEHICLE_MAKE\")))\\\n",
    "                    .withColumn(\"VMRANK\", F.dense_rank().over(Window.partitionBy().orderBy(desc(\"VMCNT\"))))\\\n",
    "                    .withColumn(\"VEHICLE_MAKE\", when(col(\"VMRANK\") <= 15 ,col(\"VEHICLE_MAKE\"))\\\n",
    "                                .otherwise(\"OTHER MAKE\"))\\\n",
    "                    .withColumn(\"VEHICLE_CHANGED_DIRECTION\", when(col(\"INITIAL_DIRECTION\") == col(\"FINAL_DIRECTION\"),\"No\")\\\n",
    "                                .when(col(\"INITIAL_DIRECTION\") == 'NK',\"Not known\")\\\n",
    "                                .when(col(\"FINAL_DIRECTION\") == 'NK',\"Not known\")\\\n",
    "                                     .otherwise(\"Yes\"))\\\n",
    "                    .withColumn(\"VEHICLE_COLOUR_1\",when(col(\"VEHICLE_COLOUR_1\") == \"BLK\", \"Black\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_1\") == \"BLU\", \"Blue\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_1\") == \"BRN\", \"Brown\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_1\") == \"CRM\", \"Cream\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_1\") == \"FWN\", \"Fawn\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_1\") == \"GLD\", \"Gold\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_1\") == \"GRN\", \"Green\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_1\") == \"GRY\", \"Grey\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_1\") == \"MRN\", \"Maroon\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_1\") == \"MVE\", \"Mauve\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_1\") == \"OGE\", \"Orange\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_1\") == \"PNK\", \"Pink\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_1\") == \"PUR\", \"Purple\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_1\") == \"RED\", \"Red\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_1\") == \"SIL\", \"Silver\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_1\") == \"WHI\", \"White\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_1\") == \"YLW\", \"Yellow\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_1\") == \"ZZ\", \"Unknown or N/A\")\\\n",
    "                            .otherwise(\"Unknown or N/A\"))\\\n",
    "                    .withColumn(\"VEHICLE_COLOUR_2\",when(col(\"VEHICLE_COLOUR_2\") == \"BLK\", \"Black\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_2\") == \"BLU\", \"Blue\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_2\") == \"BRN\", \"Brown\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_2\") == \"CRM\", \"Cream\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_2\") == \"FWN\", \"Fawn\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_2\") == \"GLD\", \"Gold\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_2\") == \"GRN\", \"Green\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_2\") == \"GRY\", \"Grey\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_2\") == \"MRN\", \"Maroon\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_2\") == \"MVE\", \"Mauve\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_2\") == \"OGE\", \"Orange\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_2\") == \"PNK\", \"Pink\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_2\") == \"PUR\", \"Purple\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_2\") == \"RED\", \"Red\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_2\") == \"SIL\", \"Silver\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_2\") == \"WHI\", \"White\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_2\") == \"YLW\", \"Yellow\")\\\n",
    "                            .when(col(\"VEHICLE_COLOUR_2\") == \"ZZ\", \"Unknown or N/A\")\\\n",
    "                            .otherwise(\"Unknown or N/A\"))\\\n",
    "                    .sort(\"ACCIDENT_NO\")\\\n",
    "                    .select(\"ACCIDENT_NO\",\"VEHICLE_ID\",\"VEHICLE_YEAR_MANUF\", \"VEHICLE_DCA_CODE\", \"VEHICLE_DCA_DESC\", \n",
    "                            \"ROAD_SURFACE_TYPE\",\"ROAD_SURFACE_TYPE_DESC\",\"REG_STATE_GRP\",\n",
    "                            \"VEHICLE_BODY_STYLE\",\"VEHICLE_MAKE\",\"VEHICLE_MODEL\",\"VEHICLE_TYPE\",\n",
    "                            \"VEHICLE_TYPE_DESC\",\"FUEL_TYPE\",\"FUEL_TYPE_DESC\",\n",
    "                            \"TOTAL_NO_OCCUPANTS\",\"DRIVER_INTENT\", \"DRIVER_INTENT_DESC\",\n",
    "                            \"VEHICLE_MOVEMENT\",\"VEHICLE_MOVEMENT_DESC\",\"TRAILER_TYPE\",\"TRAILER_TYPE_DESC\",\n",
    "                            \"VEHICLE_COLOUR_1\",\"VEHICLE_COLOUR_2\",\"CAUGHT_FIRE\",\"INITIAL_IMPACT\",\"INITIAL_IMPACT_DESC\",\n",
    "                            \"LAMPS\",\"LAMPS_ON\",\"LEVEL_OF_DAMAGE\",\"LEVEL_OF_DAMAGE_DESC\",\"OWNER_POSTCODE\",\n",
    "                            \"TRAFFIC_CONTROL\",\"TRAFFIC_CONTROL_DESC\",\"VEHICLE_CHANGED_DIRECTION\")\\\n",
    "                    .cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of records & number of columns after transformation\n",
    "ndf_vehicle_count = ndf_vehicle.count()\n",
    "ndf_vehicle_len = len(ndf_vehicle.columns)\n",
    "print(\"Number of Records: \", ndf_vehicle_count, \" Number of columns: \", ndf_vehicle_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result\n",
    "ndf_vehicle.show(2,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shape_clean = spark.createDataFrame(\n",
    "    [\n",
    "        (\"df_accident\", \"Number of Records\", ndf_accident_count ), \n",
    "        (\"df_accident\", \"Number of Columns\", ndf_accident_len), \n",
    "        (\"df_atmospheric_cond\", \"Number of Records\", ndf_atmospheric_cond_count), \n",
    "        (\"df_atmospheric_cond\", \"Number of Columns\", ndf_atmospheric_cond_len), \n",
    "        (\"df_node\", \"Number of Records\", ndf_node_count ), \n",
    "        (\"df_node\", \"Number of Columns\", ndf_node_len), \n",
    "        (\"df_person\", \"Number of Records\", ndf_person_count ), \n",
    "        (\"df_person\", \"Number of Columns\", ndf_person_len), \n",
    "        (\"df_surface_cond\", \"Number of Records\", ndf_surface_cond_count ), \n",
    "        (\"df_surface_cond\", \"Number of Columns\", ndf_surface_cond_len), \n",
    "        (\"df_vehicle\", \"Number of Records\", ndf_vehicle_count), \n",
    "        (\"df_vehicle\", \"Number of Columns\", ndf_vehicle_len), \n",
    "    ],\n",
    "    [\"DATAFRAME\", \"SHAPE\",\"TRANSFORM RESULTS\"] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary\n",
    "# check the shape of the clean dataframe\n",
    "df_shape_clean.groupBy(\"DATAFRAME\").pivot(\"SHAPE\").sum().sort(\"Number of Records\",\"Number of Columns\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear Cache for base dataframes\n",
    "df_accident.unpersist()\n",
    "df_accident_event.unpersist()\n",
    "df_atmospheric_cond.unpersist()\n",
    "df_node.unpersist()\n",
    "df_node_id.unpersist()\n",
    "df_person.unpersist()\n",
    "df_subdca.unpersist()\n",
    "df_surface_cond.unpersist()\n",
    "df_vehicle.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"transform_part2\"></a>\n",
    "<div style=\"background:rgba(0,80,80,0.2);padding:10px;border-radius:4px\"><h3>Part 2: Join all the clean dataframes into a joined dataframe </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the performance, based on the *number of records and matching keys* between dataframes the join operations will be split into different stages:  \n",
    "1. Join operation stage 1: `ndf_accident` + `ndf_atmospheric_cond` + `ndf_surface_cond` + `ndf_node`\n",
    "2. Join operation stage 2: `ndf_person` + `ndf_vehicle`\n",
    "3. Join operation final stage: join result from Step 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check any performance fine-tuning required\n",
    "# performance fine-tuning in partition is requried if data is skewed\n",
    "# TURN OFF if no action is required.\n",
    "\n",
    "# check the partition distribution\n",
    "#ndf_accident.withColumn(\"partitionId\", spark_partition_id()).groupBy(\"partitionId\").count().orderBy(asc(\"count\")).show()\n",
    "#ndf_atmospheric_cond.withColumn(\"partitionId\", spark_partition_id()).groupBy(\"partitionId\").count().orderBy(asc(\"count\")).show()\n",
    "#ndf_surface_cond.withColumn(\"partitionId\", spark_partition_id()).groupBy(\"partitionId\").count().orderBy(asc(\"count\")).show()\n",
    "#ndf_node.withColumn(\"partitionId\", spark_partition_id()).groupBy(\"partitionId\").count().orderBy(asc(\"count\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare aliases \n",
    "a = ndf_accident.alias('a')\n",
    "ac = ndf_atmospheric_cond.alias('ac')\n",
    "sc = ndf_surface_cond.alias('sc')\n",
    "n = ndf_node.alias('n')\n",
    "p = ndf_person.alias('p')\n",
    "v = ndf_vehicle.alias('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# composite keys condition\n",
    "cond1 = [a.ACCIDENT_NO == n.ACCIDENT_NO, a.NODE_ID == n.NODE_ID] \n",
    "\n",
    "## Join Operation 1\n",
    "# dataset a,ac,sc have the same number of records, inner join is used\n",
    "# dataset n has less number of records than a,ac,sc\n",
    "# in dataset a contains invalid NODE_ID,  outer join is used to remove invalid NODE_ID in order to avoid correct accident's records are removed, inner join is NOT used\n",
    "# node_id is not provided. Fill records with None/0(placeholder for missing node info) when NODE_ID is not provided.\n",
    "\n",
    "# drop duplicate columns( join keys)\n",
    "df_crash_stage1 = a.join(ac, a.ACCIDENT_NO == ac.ACCIDENT_NO, how='inner')\\\n",
    "                    .join(sc, a.ACCIDENT_NO == sc.ACCIDENT_NO, how='inner')\\\n",
    "                    .join(n, cond1, how='left')\\\n",
    "                    .drop(ac[\"ACCIDENT_NO\"]).drop(sc[\"ACCIDENT_NO\"]).drop(n[\"ACCIDENT_NO\"]).drop(a[\"NODE_ID\"])\\\n",
    "                    .fillna({\"NODE_ID\": 0}).fillna('None')\\\n",
    "                    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#composite keys condition\n",
    "cond2 = [p.ACCIDENT_NO == v.ACCIDENT_NO, p.VEHICLE_ID == v.VEHICLE_ID] \n",
    "\n",
    "# Join Operation 2\n",
    "# Left operation is used because \n",
    "# 1. not all people involved in accidents are in vehicles\n",
    "# 2. sometime information is not collected by the police department.\n",
    "\n",
    "df_crash_stage2 = p.join(v, cond2, how='left')\\\n",
    "                    .drop(v[\"ACCIDENT_NO\"]).drop(v[\"VEHICLE_ID\"])\\\n",
    "                    .fillna('N/A').fillna(0)\\\n",
    "                    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc1 = df_crash_stage1.alias('dfc1') # contain infomation about accidents\n",
    "dfc2 = df_crash_stage2.alias('dfc2') # contain people and vehicle involved in the accidents\n",
    "\n",
    "# Final stage join operations\n",
    "#df_crash_temp1 = dfc2.join(broadcast(dfc1), dfc2.ACCIDENT_NO == dfc1.ACCIDENT_NO, how = \"inner\" )\\\n",
    "df_crash_temp1 = dfc2.join(dfc1, dfc2.ACCIDENT_NO == dfc1.ACCIDENT_NO, how = \"inner\" )\\\n",
    "        .drop(dfc1[\"ACCIDENT_NO\"])\n",
    "       # .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolate step for logs\n",
    "#df_crash_temp1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result\n",
    "#df_crash_temp1.limit(3).toPandas()\n",
    "df_crash_temp1.show(1, vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of records & number of columns after transformation\n",
    "df_crash_stage1_count = df_crash_stage1.count()\n",
    "df_crash_stage2_count = df_crash_stage2.count()\n",
    "df_crash_final_count = df_crash_temp1.count()\n",
    "df_crash_final_len = len(df_crash_temp1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of output: check the number of records before & after join\n",
    "print(\"Number of Records\")\n",
    "print(\"Stage 1    | Target : \", ndf_accident_count ,\" After join result : \", df_crash_stage1_count)\n",
    "print(\"Stage 2    | Target : \", ndf_person_count ,\" After join result : \", df_crash_stage2_count)\n",
    "print(\"Final Stage| Target : \", df_crash_stage2_count,\" After join result : \", df_crash_final_count)\n",
    "print(\" Number of columns: \", df_crash_final_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result\n",
    "df_crash_temp1.limit(1).show(1,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check invalid NODE_ID -1, -3,-10 whether has been removed\n",
    "# -1 is used as placeholder for missing node's information\n",
    "df_crash_stage1.filter(col('NODE_ID').isin('-1','-3','-10')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache for cleaned base Dataframes no longer being referred to below:\n",
    "ndf_accident.unpersist()\n",
    "ndf_atmospheric_cond.unpersist()\n",
    "ndf_node.unpersist()\n",
    "ndf_person.unpersist()\n",
    "ndf_surface_cond.unpersist()\n",
    "ndf_vehicle.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"transform_part3\"></a>\n",
    "<div style=\"background:rgba(0,80,80,0.2);padding:10px;border-radius:4px\"><h3>Part 3: Perform exploration, cleaning & transformation on the joined dataframe </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Vehicle Age - Based on Year of Manufacture from Vehicle, and the Accident date from Accident.\n",
    "df_crash_temp2 = df_crash_temp1.withColumn(\"VEHICLE_AGE_YRS\", \n",
    "                                           when(col(\"VEHICLE_YEAR_MANUF\").isNotNull(), (col(\"ACCIDENTDATE\")[-4:4])-col(\"VEHICLE_YEAR_MANUF\"))\\\n",
    "                                           .otherwise(\"Unknown\"))\\\n",
    "                                .withColumn(\"VEHICLE_AGE_GROUP\", \n",
    "                                     when(( col(\"VEHICLE_AGE_YRS\").isNull()),\"Unknown\")\\\n",
    "                                    .when((( col(\"VEHICLE_AGE_YRS\") >= 0) & (col(\"VEHICLE_AGE_YRS\") <= 3)),\"0-3\")\\\n",
    "                                    .when((( col(\"VEHICLE_AGE_YRS\") >= 4) & (col(\"VEHICLE_AGE_YRS\") <= 7)),\"4-7\")\\\n",
    "                                    .when((( col(\"VEHICLE_AGE_YRS\") >= 8) & (col(\"VEHICLE_AGE_YRS\") <= 10)),\"8-10\")\\\n",
    "                                    .when(( col(\"VEHICLE_AGE_YRS\") >= 11),\"11+\")\\\n",
    "                                    .otherwise(\"Unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display result\n",
    "df_crash_temp2.select(\"ACCIDENTDATE\",\"VEHICLE_YEAR_MANUF\",\"VEHICLE_AGE_YRS\",\"VEHICLE_AGE_GROUP\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMAT cleaned dataset for exporting\n",
    "\n",
    "# rename the description and select relevant attributes\n",
    "## remove key id\n",
    "## rename attributes name without \"DESC\"\n",
    "\n",
    "# re-organise the columns sequence\n",
    "## Records ID\n",
    "## Classification field\n",
    "## ACCIDENT + ROAD SURFACE + ATMOSPHERIC\n",
    "## PERSON\n",
    "## NODE\n",
    "## VEHICLE\n",
    "\n",
    "df_crash_final = df_crash_temp2.select(\"ACCIDENT_NO\",\"PERSON_ID\", \"NODE_ID\", \"VEHICLE_ID\",\n",
    "                                       col(\"SEVERITY_DESC\").alias(\"SEVERITY\"), col(\"INJ_LEVEL_DESC\").alias(\"INJ_LEVEL\"),\n",
    "                                       \"ACCIDENT_MONTH\", col(\"ACCIDENT_HOUR_GROUP\").alias(\"ACCIDENT_HOUR\"), col(\"DAY_OF_WEEK_DESC\").alias(\"DAY_OF_WEEK\"), col(\"ACCIDENT_TYPE_DESC\").alias(\"ACCIDENT_TYPE\"), col(\"DCA_DESC\").alias(\"DCA\"),col(\"LIGHT_CONDITION_DESC\").alias(\"LIGHT_COND\"),col(\"ROAD_GEOMETRY_DESC\").alias(\"ROAD_GEOMETRY\"),\"POLICE_ATTEND\",col(\"SPEED_ZONE_DESC\").alias(\"SPEED_ZONE\"),\n",
    "                                       \"SURFACE_DRY\",\"SURFACE_WET\",\"SURFACE_MUDDY\", \"SURFACE_SNOWY\",\"SURFACE_ICY\",\"SURFACE_UNKNOWN\",\n",
    "                                       \"ATMOSPH_CLEAR\",\"ATMOSPH_RAINING\",\"ATMOSPH_SNOWING\",\"ATMOSPH_FOG\",\"ATMOSPH_SMOKE\",\"ATMOSPH_DUST\",\"ATMOSPH_STRONG_WINDS\",\"ATMOSPH_UNKNOWN\",\n",
    "                                       col(\"NODE_TYPE_DESC\").alias(\"NODE_TYPE\"),\"LGA_NAME\",\"REGION_NAME\",\"DEG_URBAN_NAME\", \n",
    "                                       \"SEX\",\"AGE_GROUP\",\"LICENCE_STATE\",col(\"ROAD_USER_TYPE_DESC\").alias(\"ROAD_USER_TYPE\"),\n",
    "                                       \"TAKEN_HOSPITAL\",\"EJECTED_CODE\",col(\"SEATING_POSITION_DESC\").alias(\"SEATING_POSITION\"),col(\"HELMET_BELT_WORN_DESC\").alias(\"HELMET_BELT_WORN\"),col(\"PEDEST_MOVEMENT_DESC\").alias(\"PEDEST_MOVEMENT\"),\n",
    "                                       \"VEHICLE_AGE_GROUP\", \"REG_STATE_GRP\",\"VEHICLE_TYPE_DESC\",\"FUEL_TYPE_DESC\",\n",
    "                                       \"VEHICLE_MAKE\",col(\"TRAILER_TYPE_DESC\").alias(\"TRAILER_TYPE\"), \"VEHICLE_COLOUR_1\",\"VEHICLE_COLOUR_2\"  ,                                                        \n",
    "                                       col(\"VEHICLE_DCA_DESC\").alias(\"VEHICLE_DCA\"),col(\"VEHICLE_MOVEMENT_DESC\").alias(\"VEHICLE_MOVEMENT\"), \"VEHICLE_CHANGED_DIRECTION\",col(\"DRIVER_INTENT_DESC\").alias(\"DRIVER_INTENT\"),col(\"ROAD_SURFACE_TYPE_DESC\").alias(\"ROAD_SURFACE_TYPE\"),\n",
    "                                       \"TOTAL_NO_OCCUPANTS\",col(\"INITIAL_IMPACT_DESC\").alias(\"INITIAL_IMPACT\"),col(\"LEVEL_OF_DAMAGE_DESC\").alias(\"LEVEL_OF_DAMAGE\"),\"CAUGHT_FIRE\", \"LAMPS_ON\",col(\"TRAFFIC_CONTROL_DESC\").alias(\"TRAFFIC_CONTROL\"))\\\n",
    "                                 .cache()\n",
    "\n",
    "\n",
    "#Clear unrequired Cache of Join stage dfs\n",
    "df_crash_stage1.unpersist()\n",
    "df_crash_stage2.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confirm Schema / Data Types are as expected after joins\n",
    "df_crash_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Statistics\n",
    "df_crash_final.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinct Values for each column\n",
    "df_crash_final.select([F.countDistinct(c).alias(c) for c in df_crash_final.columns]).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing/null values : df_crash_final\n",
    "tpdf = df_crash_final.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_crash_final.columns]).toPandas().transpose()\n",
    "tpdf[(tpdf > 0).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicate\n",
    "df_crash_final.groupBy(\"ACCIDENT_NO\",\"PERSON_ID\").count().filter(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Column distribution analysis\n",
    "import matplotlib.pyplot as plt\n",
    "plt.close(\"all\")\n",
    "\n",
    "# Plot Distribution for COlumns with small enought number of distinct values for high level analysis of data.\n",
    "catg = [[\"LEVEL_OF_DAMAGE\",\"barh\"],[\"ACCIDENT_MONTH\",\"bar\"], [\"VEHICLE_MAKE\",\"bar\"],[\"INITIAL_IMPACT\",\"barh\"],\n",
    "        [\"TRAFFIC_CONTROL\",\"bar\"],[\"AGE_GROUP\",\"bar\"],[\"HELMET_BELT_WORN\",\"bar\"],[\"VEHICLE_COLOUR_1\",\"bar\"],\n",
    "        [\"SPEED_ZONE\",\"bar\"]]\n",
    "\n",
    "fig, axs = plt.subplots(3, 3,figsize=(15,10))\n",
    "\n",
    "for r in [0,1,2] :\n",
    "    for c in [0,1,2] :\n",
    "        if len(catg) == 0:\n",
    "            break\n",
    "        else :\n",
    "            cur = catg[0][0]\n",
    "            bar= catg[0][1]\n",
    "            sdf = df_crash_final.groupBy(cur).count()\n",
    "            sdf.toPandas().set_index(cur).plot(kind=bar,ax=axs[r,c],legend=False)\n",
    "            catg.pop(0)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.75,hspace=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unqiue values for each column\n",
    "[df_crash_final.groupBy(c).count().sort(col(\"count\").desc()).limit(20).show(truncate=False) for c in df_crash_final.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result\n",
    "df_crash_final.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of records & number of columns after final transformation\n",
    "df_crash_final_count = df_crash_final.count()\n",
    "df_crash_final_len = len(df_crash_final.columns)\n",
    "\n",
    "# store the transformation result\n",
    "df_shape_transform = spark.createDataFrame(\n",
    "    [\n",
    "        (\"df_crash_final\", \"Number of Records\", df_crash_final_count ), \n",
    "        (\"df_crash_final\", \"Number of Columns\", df_crash_final_len), \n",
    "    ],\n",
    "    [\"DATAFRAME\", \"SHAPE\",\"TRANSFORM RESULTS\"] \n",
    ")\n",
    "df_shape_transform.toPandas()  #.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of severity and injury\n",
    "df_crash_final.groupBy('SEVERITY','INJ_LEVEL').count().sort('SEVERITY','INJ_LEVEL').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"transformation_summary\"></a>\n",
    "<div style=\"background:rgba(0,80,80,0.2);padding:10px;border-radius:4px\"><h3> Summary</h3>\n",
    "<hr/>\n",
    "This section provide a summary of data cleaning and transformation's result. The activity includes:  <br>\n",
    " <br> - compare the total number of records and columns from different stages: Data Loading, Data Cleaning and Transformation   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the shape results from 3-activities: Data Loading, Data Cleaning and Transformation\n",
    "\n",
    "# declare alias\n",
    "i = df_shape_initial.alias('i')\n",
    "c = df_shape_clean.alias('c')\n",
    "t = df_shape_transform.alias('t')\n",
    "\n",
    "# specify join condition\n",
    "cond3 = [i.DATAFRAME == c.DATAFRAME, i.SHAPE == c.SHAPE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join these three shape's result\n",
    "# using outer join to avoid removing data in the initial dataset\n",
    "# clean the duplicated columns into a single column : DATAFRAME, SHAPE\n",
    "# pivot the result for easy comparison\n",
    "\n",
    "c.union(t).join(i, cond3, how ='outer')\\\n",
    "    .withColumn('M_DATAFRAME', \n",
    "                when(col('c.DATAFRAME').isNull(), i.DATAFRAME)\\\n",
    "                .when(col('i.DATAFRAME').isNull(), c.DATAFRAME)\\\n",
    "                .otherwise(col('i.DATAFRAME')))\\\n",
    "    .withColumn('M_SHAPE', \n",
    "                when(col('c.SHAPE').isNull(), i.SHAPE)\\\n",
    "                .when(col('i.SHAPE').isNull(), c.SHAPE)\\\n",
    "                .otherwise(col('i.SHAPE')))\\\n",
    "    .drop(c[\"DATAFRAME\"]).drop(i[\"DATAFRAME\"]).drop(c[\"SHAPE\"]).drop(i[\"SHAPE\"])\\\n",
    "    .withColumnRenamed(\"M_DATAFRAME\", \"DATAFRAME\")\\\n",
    "    .withColumnRenamed(\"M_SHAPE\", \"SHAPE\")\\\n",
    "    .select(\"DATAFRAME\",\"SHAPE\",\"INITIAL RESULTS\",\"TRANSFORM RESULTS\")\\\n",
    "    .groupBy('DATAFRAME').pivot(\"SHAPE\").sum()\\\n",
    "    .withColumnRenamed(\"Number of Columns_sum(INITIAL RESULTS)\", \"Initial_Columns\")\\\n",
    "    .withColumnRenamed(\"Number of Columns_sum(TRANSFORM RESULTS)\", \"Transform_Columns\")\\\n",
    "    .withColumnRenamed(\"Number of Records_sum(INITIAL RESULTS)\", \"Initial_Records\")\\\n",
    "    .withColumnRenamed(\"Number of Records_sum(TRANSFORM RESULTS)\", \"Transform_Records\")\\\n",
    "    .sort('Transform_Records', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 11-files provided by VicRoads, in which only **6-datasets/files** are relevant for the scope of modelling. After cleaning and transformation, only **61-attributes** will be used for modelling. The cleaned dataset consists of people and vechile that involved in accidents. Not all accidents involve vehicles (e.g. pedestrian), therefore there are `null` values for vehicle records. Moreover, these data collection inputs are rely on police force manual inputs, certain infomation might not collected. So, those data are either leave it blank or mark as unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance fine-tuning**  \n",
    "As part of big data processing, the performance of the execution been constantly monitored. We perform data skewness check in the partition, perform performance testing between joins (*sort-merge* and *broadcast-join*), and perform `cache()` or `persist()` whenever is appropriate to ensure the best efficient implementation. \n",
    "\n",
    "**toPandas()**  \n",
    "One observation that was identified was that using ToPandas() is less efficient for implementation, particularly without restricting the output first. For the purposes of assignment's submission (to achieve readable clean layout), we have adjusted few outputs to Pandas frame. We understand `.show()`, .`take()` or `.collect()` are much more efficient alternatives for implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"export_final\"></a>\n",
    "<div style=\"background:rgba(0,80,80,0.2);padding:10px;border-radius:4px\"><h2>Export cleaned/reshaped dataset</h2>\n",
    "<hr/>\n",
    "This section focuses on exporting the cleaned joined dataframe to csv/parquet and close the spark session. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to files\n",
    "\n",
    "# Parquet format - Not used for phase 2 submission, but leave here for later\n",
    "df_crash_final.write.mode(\"overwrite\").parquet(\"clean_data/df_crash_final.parquet\")\n",
    "\n",
    "#csv\n",
    "#df_crash_final.write.mode(\"overwrite\").csv(\"clean_data/df_crash_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop spark session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
